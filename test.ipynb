{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 251,
   "id": "8660b973",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from PIL import Image\n",
    "from transformers import AutoImageProcessor, AutoModelForImageClassification\n",
    "import torch\n",
    "import os\n",
    "import cv2\n",
    "import torchvision.models as models\n",
    "import torch.nn as nn\n",
    "from torchvision import transforms\n",
    "import urllib\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "id": "8b52339d",
   "metadata": {},
   "outputs": [],
   "source": [
    "script_dir = os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "id": "aaa14fb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from transformers import AutoImageProcessor, AutoModelForImageClassification\n",
    "\n",
    "class EMOTION_NET(nn.Module):\n",
    "    def __init__(self, pretrained_name=\"trpakov/vit-face-expression\", with_softmax=True):\n",
    "        super(EMOTION_NET, self).__init__()\n",
    "        self.with_softmax = with_softmax\n",
    "\n",
    "        # Load pretrained Hugging Face model\n",
    "        self.processor = AutoImageProcessor.from_pretrained(pretrained_name)\n",
    "        self.model = AutoModelForImageClassification.from_pretrained(pretrained_name)\n",
    "\n",
    "        # Get number of emotion classes from pretrained model\n",
    "        self.n_classes = self.model.config.num_labels\n",
    "\n",
    "        # Define activation\n",
    "        if self.n_classes == 1:\n",
    "            self.activation = nn.Sigmoid()\n",
    "        else:\n",
    "            self.activation = nn.Softmax(dim=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x is expected as torch tensors already preprocessed to (B, C, H, W)\n",
    "\n",
    "        # Run through pretrained model\n",
    "        outputs = self.model(x)  # logits\n",
    "        logits = outputs.logits\n",
    "\n",
    "        # Apply softmax/sigmoid if requested\n",
    "        if self.with_softmax:\n",
    "            return self.activation(logits)\n",
    "        else:\n",
    "            return logits\n",
    "\n",
    "\n",
    "# WHEN TRAINING, pass images like this: \n",
    "# inputs = emotion_net.processor(images=raw_images, return_tensors=\"pt\")\n",
    "# outputs = emotion_net(inputs[\"pixel_values\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "id": "6633a7a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision import datasets, transforms\n",
    "\n",
    "# Define preprocessing\n",
    "fer_transform = transforms.Compose([\n",
    "    transforms.Grayscale(num_output_channels=1),   # ensure 1 channel\n",
    "    transforms.Resize((48, 48)),                   # FER2013 size\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5,), (0.5,))           # normalize to [-1,1]\n",
    "])\n",
    "\n",
    "# Load the dataset (subfolders = classes)\n",
    "emotion_datasets = {\n",
    "    \"train\": datasets.ImageFolder(root=\"data/fer2013/train\", transform=fer_transform),\n",
    "    \"test\":  datasets.ImageFolder(root=\"data/fer2013/test\",  transform=fer_transform),\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "id": "9d32e9c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['angry', 'disgust', 'fear', 'happy', 'neutral', 'sad', 'surprise']\n"
     ]
    }
   ],
   "source": [
    "emotion_classes = emotion_datasets[\"test\"].classes\n",
    "print(emotion_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "id": "ad6c1fa1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 48, 48]) 0\n"
     ]
    }
   ],
   "source": [
    "img, label = emotion_datasets[\"test\"][0]\n",
    "print(img.shape, label)\n",
    "# So emotion_datasets has the form of (img, label) just like the MNIST example"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d57785d1",
   "metadata": {},
   "source": [
    "# Wrap for deepproblog"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "id": "f57cbc05",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "class EMOTION_Images(object):\n",
    "    def __init__(self, subset):\n",
    "        self.subset = subset\n",
    "\n",
    "    def __getitem__(self, item):\n",
    "        img, _ = emotion_datasets[self.subset][int(item[0])]\n",
    "        if isinstance(img, list):\n",
    "            img = torch.tensor(img)\n",
    "        return img\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "id": "1ed06f91",
   "metadata": {},
   "outputs": [],
   "source": [
    "EMOTION_train = EMOTION_Images(\"train\")\n",
    "EMOTION_test = EMOTION_Images(\"test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "id": "20d1e089",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[-0.6078, -0.7490, -0.8824,  ...,  0.0431,  0.1843, -0.3255],\n",
      "         [-0.5529, -0.7333, -0.8275,  ...,  0.0824,  0.1843, -0.3020],\n",
      "         [-0.5216, -0.7647, -0.8118,  ...,  0.1137,  0.1686, -0.3020],\n",
      "         ...,\n",
      "         [-0.1922, -0.2157, -0.2157,  ...,  0.1686, -0.1843, -0.3333],\n",
      "         [-0.1608, -0.1294, -0.1137,  ...,  0.1843, -0.0588, -0.3255],\n",
      "         [-0.1843, -0.1843, -0.1216,  ...,  0.1216,  0.0667, -0.3490]]])\n"
     ]
    }
   ],
   "source": [
    "img = EMOTION_train[[0]]\n",
    "print(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 260,
   "id": "c8ba2e5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "pl_header = f\"nn(emotion_net,[Image],Emotion,{emotion_classes}) :: face(Image, Emotion).\"\n",
    "rules = \"\"\"\n",
    "happy(Image) :- face(Image, 'happy').\n",
    "\"\"\"\n",
    "\n",
    "with open('./emotion_model.pl', \"w\") as f:\n",
    "    f.write(pl_header)\n",
    "    f.write(\"\\n\")\n",
    "    f.write(rules)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 261,
   "id": "beeca964",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fetching 1 files: 100%|██████████| 1/1 [00:00<?, ?it/s]\n"
     ]
    }
   ],
   "source": [
    "from deepproblog.network import Network\n",
    "emotion_net = EMOTION_NET()\n",
    "emotion_network = Network(emotion_net, \"emotion_net\", batching=True)\n",
    "emotion_network.optimizer = torch.optim.Adam(emotion_network.parameters(), lr=1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 262,
   "id": "d9c861b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from deepproblog.model import Model\n",
    "from deepproblog.engines import ExactEngine, ApproximateEngine\n",
    "\n",
    "emotion_model = Model(\"emotion_model.pl\", [emotion_network])\n",
    "\n",
    "# Attach a solver\n",
    "emotion_model.set_engine(ExactEngine(emotion_model))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 263,
   "id": "565e3814",
   "metadata": {},
   "outputs": [],
   "source": [
    "emotion_model.add_tensor_source(\"train\", EMOTION_train)\n",
    "emotion_model.add_tensor_source(\"test\", EMOTION_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d6e31ea",
   "metadata": {},
   "source": [
    "We can use the addition as that returns a social cognition operator that takes into account multiple cues and takes a weighted sum."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3dd74fa0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'torch.Tensor'> torch.Size([1, 48, 48])\n"
     ]
    }
   ],
   "source": [
    "img = EMOTION_train[[0]]\n",
    "print(type(img), img.shape)\n",
    "\n",
    "# "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "deepproblog",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
