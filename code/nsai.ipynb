{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e6c8d45",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from transformers import AutoImageProcessor, AutoModelForImageClassification\n",
    "import torch\n",
    "torch.set_grad_enabled(True)\n",
    "import os\n",
    "import torchvision.models as models\n",
    "import torch.nn as nn\n",
    "from torchvision import transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3ae6eed",
   "metadata": {},
   "outputs": [],
   "source": [
    "script_dir = os.getcwd()\n",
    "emotion_processor = AutoImageProcessor.from_pretrained(\"trpakov/vit-face-expression\")\n",
    "multi_face_model = AutoModelForImageClassification.from_pretrained(\"trpakov/vit-face-expression\")\n",
    "\n",
    "scene_weights_url = \"http://places2.csail.mit.edu/models_places365/resnet18_places365.pth.tar\"\n",
    "scene_checkpoint = torch.hub.load_state_dict_from_url(scene_weights_url, map_location=\"cpu\")\n",
    "\n",
    "original_scene_model = models.resnet18(num_classes=365)\n",
    "original_scene_model.fc = nn.Linear(original_scene_model.fc.in_features, 365)\n",
    "scene_state_dict = {k.replace(\"module.\", \"\"): v for k, v in scene_checkpoint[\"state_dict\"].items()}\n",
    "original_scene_model.load_state_dict(scene_state_dict)\n",
    "original_scene_model.eval()\n",
    "\n",
    "# Preprocessing, the standard torchvision normalization for ResNet, VGG, DenseNet...\n",
    "scene_tf = transforms.Compose([\n",
    "    transforms.Resize((256,256)),\n",
    "    transforms.CenterCrop(224),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                         std=[0.229, 0.224, 0.225])\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ac0fa0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from deepproblog.network import Network \n",
    "import torch.nn.functional as F\n",
    "\n",
    "original_hf_emotion_model = AutoModelForImageClassification.from_pretrained(\"trpakov/vit-face-expression\")\n",
    "\n",
    "class MultiFaceWrapperFlat(nn.Module):\n",
    "    def __init__(self, hf_model, max_faces=5):\n",
    "        super().__init__()\n",
    "        self.hf_model = hf_model\n",
    "        self.max_faces = max_faces\n",
    "\n",
    "    def forward(self, x):\n",
    "        if isinstance(x, list) and len(x) == 1 and isinstance(x[0], list):\n",
    "            face_tensors = x[0]\n",
    "        elif isinstance(x, list):\n",
    "            face_tensors = x\n",
    "        else:\n",
    "            face_tensors = [x]\n",
    "        \n",
    "        face_probs_list = []\n",
    "        \n",
    "        for i, face_tensor in enumerate(face_tensors[:self.max_faces]):\n",
    "            if torch.is_tensor(face_tensor):\n",
    "                face_batch = face_tensor.unsqueeze(0)\n",
    "                outputs = self.hf_model(face_batch)\n",
    "                probs = F.softmax(outputs.logits, dim=-1)\n",
    "                face_probs_list.append(probs.squeeze(0)) \n",
    "        \n",
    "        while len(face_probs_list) < self.max_faces:\n",
    "            face_probs_list.append(torch.zeros(7, requires_grad=False))\n",
    "        \n",
    "        result = torch.cat(face_probs_list, dim=0)\n",
    "        return result\n",
    "\n",
    "original_hf_emotion_model.eval()\n",
    "for p in original_hf_emotion_model.parameters():\n",
    "    p.requires_grad = False\n",
    "\n",
    "max_faces = 5\n",
    "multi_face_model = MultiFaceWrapperFlat(original_hf_emotion_model, max_faces=max_faces)\n",
    "\n",
    "# We freeze the network for now because it is already pretrained\n",
    "# Freeze multi-face (no optimizer)\n",
    "multi_face_model.eval()\n",
    "for p in multi_face_model.parameters():\n",
    "    p.requires_grad = False\n",
    "multi_face_network = Network(multi_face_model, \"multi_face_net\", batching=False)\n",
    "multi_face_network.optimizer = None  # no training\n",
    "    \n",
    "class SceneFeatureExtractor(nn.Module):\n",
    "    def __init__(self, model):\n",
    "        super().__init__()\n",
    "        self.backbone = nn.Sequential(*list(model.children())[:-1])\n",
    "\n",
    "    def forward(self, x):\n",
    "        if isinstance(x, list):\n",
    "            x = torch.stack(x, dim=0)\n",
    "        elif len(x.shape) == 3:\n",
    "            x = x.unsqueeze(0)\n",
    "            \n",
    "        feats = self.backbone(x)\n",
    "        feats = feats.view(feats.size(0), -1)\n",
    "\n",
    "        return feats\n",
    "\n",
    "scene_model = SceneFeatureExtractor(original_scene_model)\n",
    "# Freeze scene feature extractor\n",
    "scene_model.eval()\n",
    "for p in scene_model.parameters():\n",
    "    p.requires_grad = False\n",
    "scene_network = Network(scene_model, \"scene_net\", batching=True)\n",
    "scene_network.optimizer = None  # no training\n",
    "\n",
    "class Scene2EmotionNet(nn.Module):\n",
    "    def __init__(self, input_dim=512, hidden_dim=128, output_dim=7, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(input_dim, hidden_dim)\n",
    "        self.fc2 = nn.Linear(hidden_dim, output_dim)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "        # Use Kaiming initialization for ReLU\n",
    "        nn.init.kaiming_uniform_(self.fc1.weight, nonlinearity='relu')\n",
    "        nn.init.zeros_(self.fc1.bias)\n",
    "        nn.init.kaiming_uniform_(self.fc2.weight, nonlinearity='relu')\n",
    "        nn.init.zeros_(self.fc2.bias)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.dropout(x)\n",
    "        logits = self.fc2(x)\n",
    "\n",
    "        probs = F.softmax(logits, dim=-1)\n",
    "        return probs\n",
    "    \n",
    "scene2emo_model = Scene2EmotionNet()\n",
    "# Trainable adapter\n",
    "scene2emo_model.train()\n",
    "scene2emo_network = Network(scene2emo_model, \"scene2emo_net\", batching=True)\n",
    "scene2emo_network.optimizer = torch.optim.Adam(scene2emo_network.parameters(), lr=1e-5, weight_decay=1e-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e8d6aeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import random\n",
    "from io import StringIO\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import display\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "import cv2\n",
    "from PIL import Image\n",
    "\n",
    "script_dir = os.getcwd()\n",
    "labels_dir = os.path.join(script_dir, \"../data/findingemo/labels\")\n",
    "base_findingemo_dir = os.path.join(script_dir, \"../data/findingemo\")\n",
    "csv_path = os.path.join(labels_dir, \"annotations_cleaned.csv\")\n",
    "df_findingEmo = pd.read_csv(csv_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "681315f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the different emotions from FindingEmo\n",
    "emotion_labels_findingemo = df_findingEmo['emotion'].unique().tolist()\n",
    "emotion_to_idx = {emotion: idx for idx, emotion in enumerate(emotion_labels_findingemo)}\n",
    "print(emotion_labels_findingemo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d42c308",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_test split\n",
    "from sklearn.model_selection import train_test_split\n",
    "# Shuffle the dataset\n",
    "df_findingEmo = df_findingEmo.sample(frac=1, random_state=42).reset_index(drop=True)\n",
    "train_df, test_df = train_test_split(df_findingEmo, test_size=0.2, random_state=42)\n",
    "\n",
    "# reset indexes\n",
    "train_df = train_df.reset_index(drop=True)\n",
    "test_df = test_df.reset_index(drop=True)\n",
    "\n",
    "train_df = train_df.iloc[:200]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5df1fa4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from mtcnn import MTCNN\n",
    "# Image access (tensors) - separate classes for different preprocessing\n",
    "class FindingEmoFaceImages(object):\n",
    "    def __init__(self, subset, prints=False):\n",
    "        self.subset = subset\n",
    "        self.face_detector = MTCNN()\n",
    "        self.face_tensors = {}\n",
    "        self.prints = prints\n",
    "\n",
    "        if self.subset == \"train\":\n",
    "            self.dataframe = train_df.copy()\n",
    "        else:\n",
    "            self.dataframe = test_df.copy()\n",
    "# Will return a list of tensors corresponding to all the detected faces.\n",
    "# If no face is detected, just return the tensor of the image.\n",
    "    def __getitem__(self, item):\n",
    "        index = int(item[0]) if isinstance(item, (tuple, list)) else int(item)\n",
    "        \n",
    "        image_path = self.dataframe[\"image_path\"][index]\n",
    "        if image_path.startswith('/'):\n",
    "            image_path = image_path[1:]\n",
    "        \n",
    "        img_path = os.path.join(base_findingemo_dir, image_path)\n",
    "        \n",
    "        img = cv2.imread(img_path)\n",
    "        img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "        \n",
    "        # Detect the faces\n",
    "        face_results = self.face_detector.detect_faces(img)\n",
    "\n",
    "        # Sort faces from bigger to smaller\n",
    "        face_results = sorted(face_results, key=lambda x: x['box'][2] * x['box'][3], reverse=True)\n",
    "\n",
    "        # Only max faces\n",
    "        face_results = face_results[:max_faces]\n",
    "\n",
    "        face_tensors = []\n",
    "        face_pils = []\n",
    "\n",
    "        if len(face_results) > 0:\n",
    "            for face_result in face_results:\n",
    "                x, y, w, h = face_result['box']\n",
    "\n",
    "                # Handle negative coordinates and bounds\n",
    "                x = max(0, x)\n",
    "                y = max(0, y)\n",
    "                x2 = min(img.shape[1], x + w)\n",
    "                y2 = min(img.shape[0], y + h)\n",
    "\n",
    "                # Face crop\n",
    "                face_crop = img[y:y2, x:x2] \n",
    "                # Skip if the crop is too small\n",
    "                if face_crop.shape[0] < 10 or face_crop.shape[1] < 10:\n",
    "                    continue\n",
    "\n",
    "                # Converting to PIL and process it with HF processor\n",
    "                face_pil = Image.fromarray(face_crop)\n",
    "                face_pils.append(face_pil)\n",
    "                face_input = emotion_processor(images=face_pil, return_tensors=\"pt\")\n",
    "                face_tensors.append(face_input['pixel_values'].squeeze(0))\n",
    "\n",
    "        if len(face_tensors) == 0:\n",
    "            # Use whole image as fallback\n",
    "            img_pil = Image.fromarray(img)\n",
    "            face_pils.append(img_pil)\n",
    "            face_input = emotion_processor(images = img_pil, return_tensors=\"pt\")\n",
    "            face_tensors.append(face_input['pixel_values'].squeeze(0))\n",
    "        \n",
    "        if self.prints:\n",
    "            self.face_tensors[index] = face_pils\n",
    "            \n",
    "        return face_tensors\n",
    "    \n",
    "    def print_faces_and_image(self, i):\n",
    "        _ = self.__getitem__(i)\n",
    "\n",
    "        if i in self.face_tensors:\n",
    "            face_tensors = self.face_tensors[i]\n",
    "            num_faces = len(face_tensors)\n",
    "            if num_faces > 0:\n",
    "                plt.figure(figsize=(8, 3))\n",
    "            for j, face_tensor in enumerate(face_tensors):\n",
    "                plt.subplot(1, num_faces, j + 1)\n",
    "                plt.imshow(face_tensor)\n",
    "                plt.title(f'Face {j}')\n",
    "                plt.axis('off')\n",
    "            plt.show()\n",
    "\n",
    "        # Print the original image\n",
    "        image_path = self.dataframe[\"image_path\"][i]\n",
    "        if image_path.startswith('/'):\n",
    "            image_path = image_path[1:]\n",
    "        img_path = os.path.join(base_findingemo_dir, image_path)\n",
    "        img = cv2.imread(img_path)\n",
    "        img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "        plt.figure(figsize=(4, 4))\n",
    "        plt.imshow(img)\n",
    "        plt.title('Image: ' + str(i) + ' : ' + self.dataframe[\"emotion\"][i])\n",
    "        plt.axis('off')\n",
    "        plt.show()\n",
    "\n",
    "class FindingEmoSceneImages(object):\n",
    "    def __init__(self, subset):\n",
    "        self.subset = subset\n",
    "        if self.subset == \"train\":\n",
    "            self.dataframe = train_df.copy()\n",
    "        else:\n",
    "            self.dataframe = test_df.copy()\n",
    "            \n",
    "    def __getitem__(self, item):\n",
    "        index = int(item[0]) if isinstance(item, (tuple, list)) else int(item)\n",
    "        \n",
    "        image_path = self.dataframe[\"image_path\"][index]\n",
    "        if image_path.startswith('/'):\n",
    "            image_path = image_path[1:]\n",
    "        \n",
    "        img_path = os.path.join(base_findingemo_dir, image_path)\n",
    "        \n",
    "        if not os.path.exists(img_path):\n",
    "            raise FileNotFoundError(f\"Image not found: {img_path}\")\n",
    "            \n",
    "        img = cv2.imread(img_path)\n",
    "        if img is None:\n",
    "            raise ValueError(f\"Failed to load image: {img_path}\")\n",
    "            \n",
    "        img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "        img_pil = Image.fromarray(img)\n",
    "        \n",
    "        # Use scene transforms for scene model\n",
    "        scene_tensor = scene_tf(img_pil)\n",
    "        return scene_tensor\n",
    "\n",
    "# Create separate instances\n",
    "FindingEmoFace_train_ex = FindingEmoFaceImages(\"train\", prints=True)\n",
    "FindingEmoFace_train = FindingEmoFaceImages(\"train\")\n",
    "FindingEmoFace_test = FindingEmoFaceImages(\"test\")\n",
    "FindingEmoScene_train = FindingEmoSceneImages(\"train\") \n",
    "FindingEmoScene_test = FindingEmoSceneImages(\"test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "710cdd02",
   "metadata": {},
   "outputs": [],
   "source": [
    "emotion_categories = ['angry', 'disgust', 'fear', 'happy', 'sad', 'surprise', 'neutral']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8a48d24",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mapping for the scene categories. Read them from ../data/places365/categories_places365.txt\n",
    "scene_categories = []\n",
    "with open(os.path.join(script_dir, \"../data/places365/categories_places365.txt\")) as f:\n",
    "    for line in f:\n",
    "        scene = line.strip().split(' ')[0][3:]\n",
    "        scene_categories.append(scene)\n",
    "\n",
    "model_scene_to_index = {scene: idx for idx, scene in enumerate(scene_categories)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcdfb5e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from deepproblog.model import Model\n",
    "from deepproblog.network import Network\n",
    "from deepproblog.engines import ExactEngine\n",
    "from deepproblog.dataset import Dataset, DataLoader\n",
    "from torch.utils.data import Dataset as TorchDataset\n",
    "from problog.logic import Term, Constant, Var\n",
    "from deepproblog.query import Query\n",
    "from deepproblog.train import train_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d604eee",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_emotions = len(emotion_categories) * max_faces\n",
    "num_scenes = len(scene_categories)\n",
    "\n",
    "emotion_indices = list(range(num_emotions))\n",
    "scene_indices = list(range(num_scenes))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "921c4e95",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_indices = list(range(len(train_df))) \n",
    "test_indices = list(range(len(test_df))) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef23f7cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from deepproblog.engines import ApproximateEngine, ExactEngine\n",
    "# Create model\n",
    "model = Model(\n",
    "    os.path.join(script_dir, \"model.pl\"),\n",
    "    [multi_face_network, scene_network, scene2emo_network]\n",
    ")\n",
    "model.set_engine(ExactEngine(model), cache=True)\n",
    "# from deepproblog.engines.prolog_engine.heuristics import PartialProbability\n",
    "# model.set_engine(ApproximateEngine(model, k=1, heuristic=PartialProbability(), exploration=False))\n",
    "\n",
    "# Register tensor sources\n",
    "model.add_tensor_source(\"train_face\", FindingEmoFace_train)\n",
    "model.add_tensor_source(\"test_face\", FindingEmoFace_test)\n",
    "model.add_tensor_source(\"train_scene\", FindingEmoScene_train)\n",
    "model.add_tensor_source(\"test_scene\", FindingEmoScene_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fb769a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_sample_idx = random.randint(0, len(train_df) - 1)\n",
    "FindingEmoFace_train_ex.print_faces_and_image(train_sample_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fecdfebc",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = Query(\n",
    "    Term(\"scene_test\",\n",
    "         Term(\"tensor\", Term(\"train_scene\", Constant(train_sample_idx))),\n",
    "         Var(\"Emo\")),\n",
    "    substitution={}\n",
    ")\n",
    "\n",
    "results = model.solve([query])\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68f3fc99",
   "metadata": {},
   "outputs": [],
   "source": [
    "FindingEmoFace_train_ex.print_faces_and_image(train_sample_idx)\n",
    "\n",
    "query = Query(\n",
    "    Term(\"final_findingemo\",\n",
    "         Term(\"tensor\", Term(\"train_face\", Constant(train_sample_idx))),\n",
    "         Term(\"tensor\", Term(\"train_scene\", Constant(train_sample_idx))),\n",
    "         Var(\"FEIdx\")),\n",
    "    substitution={}\n",
    ")\n",
    "\n",
    "results_final = model.solve([query])\n",
    "results_final_dict = results_final[0].result\n",
    "\n",
    "# Sort by tensor values and display in a readable format\n",
    "sorted_results = sorted(results_final_dict.items(), key=lambda x: x[1].item(), reverse=True)\n",
    "\n",
    "print(\"Final Emotion Predictions:\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "for key, tensor_val in sorted_results:\n",
    "    # Extract emotion index from the key\n",
    "    key_str = str(key)\n",
    "    emotion_idx = int(key_str.split(\",\")[-1][:-1])\n",
    "    emotion_name = emotion_labels_findingemo[emotion_idx]\n",
    "    probability = tensor_val.item()\n",
    "    \n",
    "    print(f\"{emotion_name:15}: {probability:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da81d98f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from typing import Tuple\n",
    "\n",
    "# Dataset for training\n",
    "class FindingEmoDataset(Dataset, TorchDataset):\n",
    "    def __init__(self, dataframe, subset_name, function_name=\"final_findingemo\"):\n",
    "        super(FindingEmoDataset, self).__init__()\n",
    "        self.dataframe = dataframe.reset_index(drop=True)\n",
    "        self.subset_name = subset_name\n",
    "        self.function_name = function_name\n",
    "        self.data = list(range(len(self.dataframe)))\n",
    "    \n",
    "    def __getitem__(self, index: int) -> Tuple[int, str, int]:\n",
    "        sample_idx = self.data[index]\n",
    "        emotion_name = self.dataframe.iloc[sample_idx]['emotion']\n",
    "        emotion_idx = emotion_to_idx[emotion_name]\n",
    "        return sample_idx, emotion_name, emotion_idx\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def to_query(self, i: int) -> Query:\n",
    "        sample_idx = self.data[i]\n",
    "        emotion_idx = emotion_to_idx[self.dataframe.iloc[sample_idx]['emotion']]\n",
    "        \n",
    "        query = Query(\n",
    "            Term(self.function_name,\n",
    "                 Term(\"tensor\", Term(f\"{self.subset_name}_face\", Constant(sample_idx))),\n",
    "                 Term(\"tensor\", Term(f\"{self.subset_name}_scene\", Constant(sample_idx))),\n",
    "                 Constant(emotion_idx)),  # Ground truth\n",
    "            substitution={}\n",
    "        )\n",
    "        return query\n",
    "    \n",
    "    def get_label(self, i: int):\n",
    "        sample_idx = self.data[i]\n",
    "        emotion_name = self.dataframe.iloc[sample_idx]['emotion']\n",
    "        return emotion_to_idx[emotion_name]\n",
    "    \n",
    "    def get_emotion_name(self, i: int):\n",
    "        sample_idx = self.data[i]\n",
    "        return self.dataframe.iloc[sample_idx]['emotion']\n",
    "\n",
    "train_dataset = FindingEmoDataset(train_df, \"train\")\n",
    "test_dataset = FindingEmoDataset(test_df, \"test\")\n",
    "\n",
    "print(f\"Train dataset size: {len(train_dataset)}\")\n",
    "print(f\"Test dataset size: {len(test_dataset)}\")\n",
    "\n",
    "# Test the dataset\n",
    "sample_idx, emotion_name, emotion_idx = train_dataset[0]\n",
    "print(f\"Sample 0: idx={sample_idx}, emotion='{emotion_name}', label={emotion_idx}\")\n",
    "\n",
    "# Test query generation\n",
    "query = train_dataset.to_query(0)\n",
    "print(f\"Generated query: {query}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4eb486a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensure output folders exist\n",
    "snapshot_dir = os.path.join(script_dir, \"snapshot\")\n",
    "log_dir = os.path.join(script_dir, \"log\")\n",
    "os.makedirs(snapshot_dir, exist_ok=True)\n",
    "os.makedirs(log_dir, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa71d4c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "from deepproblog.train import train_model\n",
    "from deepproblog.utils.stop_condition import EpochStop\n",
    "\n",
    "loader = DataLoader(train_dataset, batch_size=1, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "483cb2f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "train = train_model(model, loader, EpochStop(1), log_iter=25, profile=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46f6c8f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from json import dumps\n",
    "from deepproblog.evaluate import get_confusion_matrix\n",
    "\n",
    "# Save results\n",
    "snapshot_path = os.path.join(snapshot_dir, \"social_cognition\" + \".pth\")\n",
    "model.save_state(snapshot_path)\n",
    "print(\"Model snapshot saved to:\", snapshot_path)\n",
    "\n",
    "log_path = os.path.join(log_dir, \"social_cognition\")\n",
    "train.logger.comment(dumps(model.get_hyperparameters()))\n",
    "train.logger.comment(\"Accuracy {}\".format(get_confusion_matrix(model, test_dataset, verbose=1).accuracy()))\n",
    "train.logger.write_to_file(log_path)\n",
    "print(\"Training log written to:\", log_path) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "774c3b07",
   "metadata": {},
   "outputs": [],
   "source": [
    "# snapshot_path = os.path.join(script_dir, \"snapshot\", \"social_cognition.pth\")\n",
    "\n",
    "# if os.path.exists(snapshot_path):\n",
    "#     print(f\"Loading model from: {snapshot_path}\")\n",
    "#     model.load_state(snapshot_path)\n",
    "# else:\n",
    "#     print(\"Error loading model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2ab8d21",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_sample_idx = random.randint(0, len(test_df) - 1)\n",
    "FindingEmoFace_test.print_faces_and_image(test_sample_idx)\n",
    "query = Query(\n",
    "    Term(\"final_findingemo\",\n",
    "         Term(\"tensor\", Term(\"test_face\", Constant(test_sample_idx))),\n",
    "         Term(\"tensor\", Term(\"test_scene\", Constant(test_sample_idx))),\n",
    "         Var(\"FEIdx\")),\n",
    "    substitution={}\n",
    ")\n",
    "\n",
    "results_final = model.solve([query])\n",
    "results_final_dict = results_final[0].result\n",
    "\n",
    "# Sort by tensor values and display in a readable format\n",
    "sorted_results = sorted(results_final_dict.items(), key=lambda x: x[1].item(), reverse=True)\n",
    "\n",
    "print(\"Enhanced Final Emotion Predictions (sorted by raw logits):\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "for key, tensor_val in sorted_results:\n",
    "    # Extract emotion index from the key\n",
    "    key_str = str(key)\n",
    "    emotion_idx = int(key_str.split(\",\")[-1][:-1])\n",
    "    emotion_name = emotion_labels_findingemo[emotion_idx]\n",
    "    probability = tensor_val.item()\n",
    "    \n",
    "    print(f\"{emotion_name:15}: {probability:.4f}\")\n",
    "\n",
    "print(f\"\\nGround Truth: {test_df.iloc[test_sample_idx]['emotion'].upper()}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "deepproblog",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
