{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "569bdee6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from transformers import AutoImageProcessor, AutoModelForImageClassification\n",
    "import torch\n",
    "import os\n",
    "import torchvision.models as models\n",
    "import torch.nn as nn\n",
    "\n",
    "import torch.optim as optim\n",
    "from torchvision import models, transforms\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from PIL import Image\n",
    "from tqdm import tqdm\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "2f958345",
   "metadata": {},
   "outputs": [],
   "source": [
    "# GLOBAL VARS\n",
    "preprocess = False\n",
    "# Put on False for full training set\n",
    "training_size = 5000\n",
    "epochs = 12\n",
    "learning_rate = 3e-3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "c722b154",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fetching 1 files: 100%|██████████| 1/1 [00:00<?, ?it/s]\n"
     ]
    }
   ],
   "source": [
    "script_dir = os.getcwd()\n",
    "face_processor = AutoImageProcessor.from_pretrained(\"trpakov/vit-face-expression\", use_fast=True)\n",
    "face_model = AutoModelForImageClassification.from_pretrained(\"trpakov/vit-face-expression\")\n",
    "face_model.eval()\n",
    "\n",
    "for p in face_model.parameters():\n",
    "    p.requires_grad = False   # frozen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "71054435",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiFaceWrapperFlat(nn.Module):\n",
    "    def __init__(self, hf_model, max_faces=3):\n",
    "        super().__init__()\n",
    "        self.hf_model = hf_model\n",
    "        self.max_faces = max_faces\n",
    "\n",
    "    def forward(self, faces):\n",
    "        \"\"\"\n",
    "        faces: list or tensor of up to 3 face tensors, each (3,224,224)\n",
    "        returns: tensor of shape (7*max_faces,) = (21,)\n",
    "        \"\"\"\n",
    "        if isinstance(faces, list):\n",
    "            faces = [f for f in faces if torch.is_tensor(f)]\n",
    "            if len(faces) == 0:\n",
    "                dummy = torch.zeros(3,224,224, device=next(self.hf_model.parameters()).device)\n",
    "                faces = [dummy]\n",
    "            faces = faces[:self.max_faces]\n",
    "            face_batch = torch.stack(faces, dim=0)\n",
    "        elif faces.ndim == 3:\n",
    "            face_batch = faces.unsqueeze(0)\n",
    "        else:\n",
    "            face_batch = faces\n",
    "\n",
    "        with torch.no_grad():\n",
    "            out = self.hf_model(face_batch)\n",
    "            logits = out.logits                   # (n_faces,7)\n",
    "        \n",
    "        # pad to max_faces\n",
    "        if logits.size(0) < self.max_faces:\n",
    "            pad = torch.zeros(self.max_faces - logits.size(0), 7, device=logits.device)\n",
    "            logits = torch.cat([logits, pad], dim=0)\n",
    "        return logits                            # (3,7)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "addc4ade",
   "metadata": {},
   "outputs": [],
   "source": [
    "multi_face_net = MultiFaceWrapperFlat(face_model, max_faces=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "bf6691a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GateMLP(nn.Module):\n",
    "    def __init__(self, in_dim, hidden=64, out_dim=4, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(in_dim, hidden),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(hidden, out_dim)\n",
    "        )\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Linear):\n",
    "                nn.init.kaiming_uniform_(m.weight, nonlinearity='relu')\n",
    "                nn.init.zeros_(m.bias)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return torch.softmax(self.net(x), dim=-1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "f68c8180",
   "metadata": {},
   "outputs": [],
   "source": [
    "gate_net = GateMLP(in_dim=12, hidden=64, out_dim=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "62fffa04",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================================================\n",
    "# 1. CONFIG\n",
    "# ==============================================================\n",
    "\n",
    "# Paths\n",
    "script_dir = os.getcwd()\n",
    "base_findingemo_dir = os.path.join(script_dir, \"../data/findingemo\")\n",
    "labels_dir = os.path.join(base_findingemo_dir, \"labels\")\n",
    "csv_path = os.path.join(labels_dir, \"annotations_cleaned.csv\")\n",
    "\n",
    "# Device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Emotion mapping (FindingEmo → 7 basic emotions)\n",
    "FindingEmo_to_basic = {\n",
    "    \"Anger\": 0, \"Rage\": 0, \"Annoyance\": 0,\n",
    "    \"Loathing\": 1, \"Disgust\": 1,\n",
    "    \"Apprehension\": 2, \"Fear\": 2, \"Terror\": 2,\n",
    "    \"Joy\": 3, \"Serenity\": 3, \"Ecstasy\": 3, \"Admiration\": 3,\n",
    "    \"Grief\": 4, \"Sadness\": 4, \"Pensiveness\": 4,\n",
    "    \"Amazement\": 5, \"Surprise\": 5,\n",
    "    \"Trust\": 6, \"Interest\": 6, \"Boredom\": 6, \"Acceptance\": 6, \"Distraction\": 6\n",
    "}\n",
    "\n",
    "# Load CSV\n",
    "df = pd.read_csv(csv_path)\n",
    "df[\"idx_emotion\"] = df[\"emotion\"].map(FindingEmo_to_basic)\n",
    "df = df.dropna(subset=[\"idx_emotion\"]).reset_index(drop=True)\n",
    "df[\"idx_emotion\"] = df[\"idx_emotion\"].astype(int)\n",
    "\n",
    "# Split\n",
    "from sklearn.model_selection import train_test_split\n",
    "train_df, val_df = train_test_split(df, test_size=0.2, random_state=42)\n",
    "train_df, val_df = train_df.reset_index(drop=True), val_df.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "804dd782",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SceneFeaturesDataset(Dataset):\n",
    "    def __init__(self, df, feature_dir):\n",
    "        self.df = df\n",
    "        self.feature_dir = feature_dir\n",
    "    def __getitem__(self, idx):\n",
    "        z = torch.load(os.path.join(self.feature_dir, f\"{idx}.pt\"))  # (365,)\n",
    "        y = torch.tensor(self.df.iloc[idx][\"idx_emotion\"], dtype=torch.long)\n",
    "        return z, y\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "dataset = SceneFeaturesDataset(df, os.path.join(base_findingemo_dir, \"precomputed/scenes\"))\n",
    "train_loader = DataLoader(dataset, batch_size=128, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "cdf912ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================================================\n",
    "# 3. MODEL: Places365 → Emotion Adapter\n",
    "# ==============================================================\n",
    "\n",
    "# Load pretrained Places365 backbone\n",
    "scene_model = models.resnet18(num_classes=365)\n",
    "weights_url = \"http://places2.csail.mit.edu/models_places365/resnet18_places365.pth.tar\"\n",
    "checkpoint = torch.hub.load_state_dict_from_url(weights_url, map_location=\"cpu\")\n",
    "state_dict = {k.replace(\"module.\", \"\"): v for k, v in checkpoint[\"state_dict\"].items()}\n",
    "scene_model.load_state_dict(state_dict)\n",
    "scene_model.eval()\n",
    "for p in scene_model.parameters():\n",
    "    p.requires_grad = False  # freeze entire backbone\n",
    "\n",
    "\n",
    "# Adapter layer 365 → 7\n",
    "class SceneToEmotionAdapter(nn.Module):\n",
    "    def __init__(self, base_scene_model, num_places=365, num_emotions=7):\n",
    "        super().__init__()\n",
    "        self.scene_model = base_scene_model\n",
    "        self.fc = nn.Linear(num_places, num_emotions)\n",
    "        nn.init.kaiming_uniform_(self.fc.weight, nonlinearity=\"relu\")\n",
    "        nn.init.zeros_(self.fc.bias)\n",
    "\n",
    "    def forward(self, x):\n",
    "        with torch.no_grad():  # freeze Places365 backbone\n",
    "            place_logits = self.scene_model(x)\n",
    "        emo_logits = self.fc(place_logits)\n",
    "        return emo_logits\n",
    "\n",
    "\n",
    "scene_net = SceneToEmotionAdapter(scene_model).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2072bd6a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1 [Train]:   0%|          | 0/468 [00:00<?, ?it/s]"
     ]
    }
   ],
   "source": [
    "# ==============================================================\n",
    "# 4. TRAINING\n",
    "# ==============================================================\n",
    "\n",
    "optimizer = optim.Adam(scene_net.fc.parameters(), lr=1e-4, weight_decay=1e-5)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "best_val_acc = 0\n",
    "for epoch in range(3):  # few epochs are enough\n",
    "    scene_net.train()\n",
    "    running_loss, correct, total = 0.0, 0, 0\n",
    "\n",
    "    for imgs, labels in tqdm(train_loader, desc=f\"Epoch {epoch+1} [Train]\"):\n",
    "        imgs, labels = imgs.to(device), labels.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        logits = scene_net(imgs)\n",
    "        loss = criterion(logits, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item() * imgs.size(0)\n",
    "        preds = logits.argmax(dim=1)\n",
    "        correct += (preds == labels).sum().item()\n",
    "        total += labels.size(0)\n",
    "\n",
    "    train_loss = running_loss / total\n",
    "    train_acc = correct / total\n",
    "\n",
    "    # Validation\n",
    "    scene_net.eval()\n",
    "    correct, total = 0, 0\n",
    "    with torch.no_grad():\n",
    "        for imgs, labels in tqdm(val_loader, desc=f\"Epoch {epoch+1} [Val]\"):\n",
    "            imgs, labels = imgs.to(device), labels.to(device)\n",
    "            logits = scene_net(imgs)\n",
    "            preds = logits.argmax(dim=1)\n",
    "            correct += (preds == labels).sum().item()\n",
    "            total += labels.size(0)\n",
    "    val_acc = correct / total\n",
    "\n",
    "    print(f\"Epoch {epoch+1}: Train Loss={train_loss:.4f}, Train Acc={train_acc:.3f}, Val Acc={val_acc:.3f}\")\n",
    "\n",
    "    if val_acc > best_val_acc:\n",
    "        best_val_acc = val_acc\n",
    "        torch.save(scene_net.state_dict(), \"scene_to_emotion_best.pth\")\n",
    "        print(\"Saved new best model.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "849c784c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Trust', 'Interest', 'Apprehension', 'Anticipation', 'Anger', 'Joy', 'Grief', 'Serenity', 'Boredom', 'Ecstasy', 'Fear', 'Vigilance', 'Sadness', 'Rage', 'Annoyance', 'Acceptance', 'Terror', 'Amazement', 'Surprise', 'Pensiveness', 'Admiration', 'Loathing', 'Distraction', 'Disgust']\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>image_path</th>\n",
       "      <th>valence</th>\n",
       "      <th>arousal</th>\n",
       "      <th>emotion</th>\n",
       "      <th>idx_emotion</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>21366</th>\n",
       "      <td>Run_2/Comforting soldiers conference/article-2...</td>\n",
       "      <td>-1</td>\n",
       "      <td>4</td>\n",
       "      <td>Ecstasy</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21368</th>\n",
       "      <td>Run_2/Compassionate seniors school/Leader-1-19...</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>Interest</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21369</th>\n",
       "      <td>Run_2/Satisfied students desert/justin-w-01.jpg</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>Admiration</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21370</th>\n",
       "      <td>Run_2/Disgusted forty-something funeral/126268...</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>Acceptance</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21371</th>\n",
       "      <td>Run_2/Irritated soldiers army/kscn0008.jpg</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>Pensiveness</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              image_path  valence  arousal  \\\n",
       "21366  Run_2/Comforting soldiers conference/article-2...       -1        4   \n",
       "21368  Run_2/Compassionate seniors school/Leader-1-19...        0        4   \n",
       "21369    Run_2/Satisfied students desert/justin-w-01.jpg        0        2   \n",
       "21370  Run_2/Disgusted forty-something funeral/126268...        0        3   \n",
       "21371         Run_2/Irritated soldiers army/kscn0008.jpg        1        4   \n",
       "\n",
       "           emotion  idx_emotion  \n",
       "21366      Ecstasy            3  \n",
       "21368     Interest            6  \n",
       "21369   Admiration            3  \n",
       "21370   Acceptance            6  \n",
       "21371  Pensiveness            4  "
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ==============================================================\n",
    "# 5. PRECOMPUTE SCENE LOGITS FOR DEEPPROBLOG\n",
    "# ==============================================================\n",
    "\n",
    "scene_net.eval()\n",
    "save_dir_train = os.path.join(base_findingemo_dir, \"precomputed/scene_logits/train\")\n",
    "save_dir_test  = os.path.join(base_findingemo_dir, \"precomputed/scene_logits/test\")\n",
    "os.makedirs(save_dir_train, exist_ok=True)\n",
    "os.makedirs(save_dir_test, exist_ok=True)\n",
    "\n",
    "def precompute_and_save(df, dataset, save_dir):\n",
    "    with torch.no_grad():\n",
    "        for i in tqdm(range(len(df)), desc=f\"Saving {save_dir}\"):\n",
    "            img, _ = dataset[i]\n",
    "            z = scene_net(img.unsqueeze(0).to(device))  # (1,7)\n",
    "            torch.save(z.squeeze(0).cpu(), os.path.join(save_dir, f\"{i}.pt\"))\n",
    "\n",
    "\n",
    "precompute_and_save(train_df, train_dataset, save_dir_train)\n",
    "precompute_and_save(val_df,   val_dataset,   save_dir_test)\n",
    "\n",
    "print(\"Done! Scene logits saved for DeepProbLog use.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5f6be1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the scene net\n",
    "# Split\n",
    "from sklearn.model_selection import train_test_split\n",
    "train_s2e, val_s2e = train_test_split(df_findingEmo, test_size=0.2, random_state=42)\n",
    "train_s2e, val_s2e = train_s2e.reset_index(drop=True), val_s2e.reset_index(drop=True)\n",
    "\n",
    "scene_tf = transforms.Compose([\n",
    "    transforms.Resize((256, 256)),\n",
    "    transforms.CenterCrop(224),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                         std=[0.229, 0.224, 0.225])\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "417a9a66",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "bc5a0603",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MoENet(nn.Module):\n",
    "    def __init__(self, face_net, scene_net, gate_net):\n",
    "        super().__init__()\n",
    "        # These are here so you *can* (optionally) run them inside MoENet\n",
    "        # and have their parameters updated end-to-end.\n",
    "        self.face_net  = face_net\n",
    "        self.scene_net = scene_net\n",
    "        self.gate_net  = gate_net\n",
    "\n",
    "    def forward(self, z_face, z_scene, face_present, num_faces):\n",
    "        # 1) logits -> probabilities (per last dim = emotion classes)\n",
    "        p_face  = torch.softmax(z_face, dim=-1)   # (B, 3, 7)\n",
    "        p_face = p_face * face_present.unsqueeze(-1)  # mask out missing ones\n",
    "        p_scene = torch.softmax(z_scene, dim=-1)  # (B, 7)\n",
    "\n",
    "        # 2) Gate features (confidence signals)\n",
    "        # Per-face max prob -> (B,3)\n",
    "        max_face  = p_face.max(dim=-1).values\n",
    "\n",
    "        # Per-face entropy -> (B,3)\n",
    "        # H(p) = -sum_k p_k log p_k\n",
    "        ent_face  = -(p_face.clamp_min(1e-12) * p_face.clamp_min(1e-12).log()).sum(dim=-1)\n",
    "\n",
    "        # Scene max prob -> (B,)\n",
    "        max_scene = p_scene.max(dim=-1).values\n",
    "\n",
    "        # Scene entropy -> (B,)\n",
    "        ent_scene = -(p_scene.clamp_min(1e-12) * p_scene.clamp_min(1e-12).log()).sum(dim=-1)\n",
    "\n",
    "        # 3) Concatenate into one gate feature vector per sample\n",
    "        # Shapes to concat along dim=-1:\n",
    "        #   max_face:     (B,3)\n",
    "        #   ent_face:     (B,3)\n",
    "        #   max_scene:    (B,) -> unsqueeze -> (B,1)\n",
    "        #   ent_scene:    (B,) -> unsqueeze -> (B,1)\n",
    "        #   face_present: (B,3)  (0/1 per face slot)\n",
    "        #   num_faces:    (B,1)\n",
    "        gate_feats = torch.cat([\n",
    "            max_face,                       # (B,3)\n",
    "            ent_face,                       # (B,3)\n",
    "            max_scene.unsqueeze(-1),        # (B,1)\n",
    "            ent_scene.unsqueeze(-1),        # (B,1)\n",
    "            face_present.float(),           # (B,3)\n",
    "            num_faces.float()               # (B,1)\n",
    "        ], dim=-1)                          # total per sample: 3+3+1+1+3+1 = 12 dims\n",
    "\n",
    "        # 4) Gate net maps features -> 4 mixing weights (faces 0,1,2, scene)\n",
    "        # Make sure your gate_net ends with a Softmax(dim=-1) so weights are >=0 and sum to 1.\n",
    "        w = self.gate_net(gate_feats)       # (B,4)\n",
    "\n",
    "        # 5) Weighted mixture of the 4 expert distributions\n",
    "        # p_face[:,0]: (B,7) = face0 distribution; etc.\n",
    "        # p_scene:      (B,7) = scene distribution\n",
    "        p_mix = (\n",
    "            w[:,0].unsqueeze(-1) * p_face[:,0] +   # weight face 0\n",
    "            w[:,1].unsqueeze(-1) * p_face[:,1] +   # weight face 1\n",
    "            w[:,2].unsqueeze(-1) * p_face[:,2] +   # weight face 2\n",
    "            w[:,3].unsqueeze(-1) * p_scene         # weight scene\n",
    "        )\n",
    "        # (Optional) Re-normalize defensively (should already sum to 1 if w sums to 1)\n",
    "        p_mix = p_mix / p_mix.sum(dim=-1, keepdim=True).clamp_min(1e-12)\n",
    "        return p_mix                           # (B,7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "f1c4825a",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'scene_net' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn [27], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m moe_model \u001b[38;5;241m=\u001b[39m MoENet(multi_face_net, \u001b[43mscene_net\u001b[49m, gate_net)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'scene_net' is not defined"
     ]
    }
   ],
   "source": [
    "moe_model = MoENet(multi_face_net, scene_net, gate_net)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "520dfec3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_test split\n",
    "from sklearn.model_selection import train_test_split\n",
    "# Shuffle the dataset\n",
    "df_findingEmo = df_findingEmo.sample(frac=1, random_state=42).reset_index(drop=True)\n",
    "train_df, test_df = train_test_split(df_findingEmo, test_size=0.2, random_state=42)\n",
    "\n",
    "# reset indexes\n",
    "train_df = train_df.reset_index(drop=True)\n",
    "test_df = test_df.reset_index(drop=True)\n",
    "\n",
    "if training_size != False:\n",
    "    train_df = train_df.iloc[:training_size]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "deepproblog",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
