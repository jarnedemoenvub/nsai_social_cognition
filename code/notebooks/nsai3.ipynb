{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "628180fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from transformers import AutoImageProcessor, AutoModelForImageClassification\n",
    "import torch\n",
    "import os\n",
    "import torchvision.models as models\n",
    "import torch.nn as nn\n",
    "from deepproblog.dataset import Dataset, DataLoader\n",
    "from torch.utils.data import Dataset as TorchDataset\n",
    "from problog.logic import Term, Constant, Var\n",
    "\n",
    "import torch.optim as optim\n",
    "from torchvision import models, transforms\n",
    "from PIL import Image\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from mtcnn import MTCNN\n",
    "import random\n",
    "from deepproblog.utils.stop_condition import EpochStop\n",
    "from deepproblog.optimizer import SGD\n",
    "\n",
    "from deepproblog.model import Model\n",
    "from deepproblog.network import Network\n",
    "from deepproblog.engines import ExactEngine\n",
    "from deepproblog.query import Query\n",
    "from deepproblog.train import train_model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "5ba3b599",
   "metadata": {},
   "outputs": [],
   "source": [
    "# GLOBAL VARS\n",
    "training_size_perc = 1\n",
    "epochs = 2\n",
    "lr_scene_reduction = 0.0001\n",
    "lr_faces = 0.0001\n",
    "lr_scenes = 0.0001\n",
    "lr_model = 0.0001\n",
    "batch_size = 16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be571aa5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "script_dir: c:\\Users\\jarne\\Documents\\Code Masterthesis\\nsai_social_cognition\\code\\notebooks\n",
      "base_dir: c:\\Users\\jarne\\Documents\\Code Masterthesis\\nsai_social_cognition\n",
      "data_dir: c:\\Users\\jarne\\Documents\\Code Masterthesis\\nsai_social_cognition\\data\n",
      "findingemo_dir: c:\\Users\\jarne\\Documents\\Code Masterthesis\\nsai_social_cognition\\data\\FindingEmo_Images\n",
      "prolog_dir: c:\\Users\\jarne\\Documents\\Code Masterthesis\\nsai_social_cognition\\code\\prolog\n"
     ]
    }
   ],
   "source": [
    "# Directories\n",
    "script_dir = os.getcwd()\n",
    "print(\"script_dir:\", script_dir)\n",
    "base_dir = os.path.dirname(os.path.dirname(script_dir))\n",
    "print(\"base_dir:\", base_dir)\n",
    "data_dir = os.path.join(base_dir, \"data\")\n",
    "print(\"data_dir:\", data_dir)\n",
    "findingemo_dir = os.path.join(data_dir, \"FindingEmo_Images\")\n",
    "print(\"findingemo_dir:\", findingemo_dir)\n",
    "prolog_dir = os.path.join(base_dir, \"code\", \"prolog\")\n",
    "print(\"prolog_dir:\", prolog_dir)\n",
    "df = pd.read_pickle(os.path.join(data_dir, \"dataframe_cleaned.pkl\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "895c7dd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "emotion_categories_hf = ['angry', 'disgust', 'fear', 'happy', 'sad', 'surprise', 'neutral']\n",
    "\n",
    "EMOTION_SETS = [[\"Serenity\", \"Joy\", \"Ecstasy\"],\n",
    "                    [\"Acceptance\", \"Trust\", \"Admiration\"],\n",
    "                    [\"Apprehension\", \"Fear\", \"Terror\"],\n",
    "                    [\"Distraction\", \"Surprise\", \"Amazement\"],\n",
    "                    [\"Pensiveness\", \"Sadness\", \"Grief\"],\n",
    "                    [\"Boredom\", \"Disgust\", \"Loathing\"],\n",
    "                    [\"Annoyance\", \"Anger\", \"Rage\"],\n",
    "                    [\"Interest\", \"Anticipation\", \"Vigilance\"]]\n",
    "\n",
    "EMO8_LIST = [l[1] for l in EMOTION_SETS]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1bdad9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Delete the columns: \"user\" and \"index\" and \"datetime\"\n",
    "if \"user\" in df.columns and \"index\" in df.columns and \"datetime\" in df.columns:\n",
    "    df = df.drop(columns=[\"user\", \"index\", \"datetime\"])\n",
    "\n",
    "# remove leading slash from image_path\n",
    "if df[\"image_path\"].iloc[0].startswith(\"/\"):\n",
    "    df[\"image_path\"] = df[\"image_path\"].str.lstrip(\"/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "9b9bfec1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply the mapping to 8 emotions as defined in the FindingEmo code\n",
    "EMO8_MAPPING = {}\n",
    "\n",
    "for i, leaf in enumerate(EMOTION_SETS):\n",
    "    for emo in leaf:\n",
    "        EMO8_MAPPING[emo] = EMO8_LIST[i]\n",
    "\n",
    "df['emotion_8'] = df['emotion'].map(EMO8_MAPPING)\n",
    "df['emotion_8_idx'] = df['emotion_8'].apply(lambda x: EMO8_LIST.index(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "71b5cbfc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>image_path</th>\n",
       "      <th>tags</th>\n",
       "      <th>age</th>\n",
       "      <th>valence</th>\n",
       "      <th>arousal</th>\n",
       "      <th>emotion</th>\n",
       "      <th>dec_factors</th>\n",
       "      <th>ambiguity</th>\n",
       "      <th>emotion_8</th>\n",
       "      <th>emotion_8_idx</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Run_2/Shame elderly sports/3364887-46.jpg</td>\n",
       "      <td>Undefined</td>\n",
       "      <td>Adults</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>Apprehension</td>\n",
       "      <td>ConflictCtxtPerson</td>\n",
       "      <td>0</td>\n",
       "      <td>Fear</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Run_2/Scared adolescents prison/15-hampton-roa...</td>\n",
       "      <td>Undefined</td>\n",
       "      <td>Youth,Adults</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>Anger</td>\n",
       "      <td>BodyLanguage,FacialExpression</td>\n",
       "      <td>0</td>\n",
       "      <td>Anger</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Run_2/Raging students party/Newly-elected-Nepa...</td>\n",
       "      <td>Undefined</td>\n",
       "      <td>Young Adults,Adults</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>Joy</td>\n",
       "      <td>BodyLanguage,FacialExpression</td>\n",
       "      <td>0</td>\n",
       "      <td>Joy</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>Run_1/Accepting children school/whitecoatcerem...</td>\n",
       "      <td>Undefined</td>\n",
       "      <td>Young Adults</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>Joy</td>\n",
       "      <td>FacialExpression</td>\n",
       "      <td>0</td>\n",
       "      <td>Joy</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>Run_1/Grateful youth protest/trees3.jpg</td>\n",
       "      <td>Undefined</td>\n",
       "      <td>Adults</td>\n",
       "      <td>-1</td>\n",
       "      <td>3</td>\n",
       "      <td>Fear</td>\n",
       "      <td>BodyLanguage,Staging</td>\n",
       "      <td>3</td>\n",
       "      <td>Fear</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                           image_path       tags  \\\n",
       "4           Run_2/Shame elderly sports/3364887-46.jpg  Undefined   \n",
       "7   Run_2/Scared adolescents prison/15-hampton-roa...  Undefined   \n",
       "8   Run_2/Raging students party/Newly-elected-Nepa...  Undefined   \n",
       "10  Run_1/Accepting children school/whitecoatcerem...  Undefined   \n",
       "11            Run_1/Grateful youth protest/trees3.jpg  Undefined   \n",
       "\n",
       "                    age  valence  arousal       emotion  \\\n",
       "4                Adults        0        3  Apprehension   \n",
       "7          Youth,Adults        0        3         Anger   \n",
       "8   Young Adults,Adults        0        2           Joy   \n",
       "10         Young Adults        0        0           Joy   \n",
       "11               Adults       -1        3          Fear   \n",
       "\n",
       "                      dec_factors  ambiguity emotion_8  emotion_8_idx  \n",
       "4              ConflictCtxtPerson          0      Fear              2  \n",
       "7   BodyLanguage,FacialExpression          0     Anger              6  \n",
       "8   BodyLanguage,FacialExpression          0       Joy              0  \n",
       "10               FacialExpression          0       Joy              0  \n",
       "11           BodyLanguage,Staging          3      Fear              2  "
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Delete all the rows that contain the emotion_8 as \"trust\" and \"anticipation\"\n",
    "for emo in [\"Trust\", \"Anticipation\"]:\n",
    "    df = df[df['emotion_8'] != emo]\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "95d959ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the different emo8's  \n",
    "basic_6_emotions = [\"Anger\", \"Disgust\", \"Fear\", \"Joy\", \"Sadness\", \"Surprise\"]\n",
    "basic_6_emotions_idx = {emo: i for i, emo in enumerate(basic_6_emotions)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "23849218",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Anger': 0, 'Disgust': 1, 'Fear': 2, 'Joy': 3, 'Sadness': 4, 'Surprise': 5}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "basic_6_emotions_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "87bbfb9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load category names\n",
    "scene_categories_path = os.path.join(data_dir, \"places365/categories_places365.txt\")\n",
    "with open(scene_categories_path) as f:\n",
    "    scene_categories = [line.strip().split(' ')[0][3:] for line in f]\n",
    "    # If there is a / in the category name, replace it with _\n",
    "    scene_categories = [cat.replace('/', '_') for cat in scene_categories]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f3d93671",
   "metadata": {},
   "outputs": [],
   "source": [
    "places365_to_13categories = {\n",
    "    # --- Transport infrastructure ---\n",
    "    \"airfield\": \"transport_infrastructure\",\n",
    "    \"airplane_cabin\": \"transport_infrastructure\",\n",
    "    \"airport_terminal\": \"transport_infrastructure\",\n",
    "    \"railroad_track\": \"transport_infrastructure\",\n",
    "    \"train_interior\": \"transport_infrastructure\",\n",
    "    \"train_station_platform\": \"transport_infrastructure\",\n",
    "    \"subway_station_platform\": \"transport_infrastructure\",\n",
    "    \"bridge\": \"transport_infrastructure\",\n",
    "    \"runway\": \"transport_infrastructure\",\n",
    "    \"highway\": \"transport_infrastructure\",\n",
    "    \"garage_indoor\": \"transport_infrastructure\",\n",
    "    \"garage_outdoor\": \"transport_infrastructure\",\n",
    "    \"bus_interior\": \"transport_infrastructure\",\n",
    "    \"bus_station_indoor\": \"transport_infrastructure\",\n",
    "    \"harbor\": \"transport_infrastructure\",\n",
    "    \"viaduct\": \"transport_infrastructure\",\n",
    "    \"heliport\": \"transport_infrastructure\",\n",
    "    \"parking_garage_indoor\": \"transport_infrastructure\",\n",
    "    \"parking_garage_outdoor\": \"transport_infrastructure\",\n",
    "    \"car_interior\": \"transport_infrastructure\",\n",
    "    \"cockpit\": \"transport_infrastructure\",\n",
    "    \"berth\": \"transport_infrastructure\",\n",
    "\n",
    "    # --- Indoor residential ---\n",
    "    \"bedroom\": \"indoor_residential\",\n",
    "    \"bathroom\": \"indoor_residential\",\n",
    "    \"living_room\": \"indoor_residential\",\n",
    "    \"kitchen\": \"indoor_residential\",\n",
    "    \"attic\": \"indoor_residential\",\n",
    "    \"childs_room\": \"indoor_residential\",\n",
    "    \"closet\": \"indoor_residential\",\n",
    "    \"dining_room\": \"indoor_residential\",\n",
    "    \"dorm_room\": \"indoor_residential\",\n",
    "    \"hotel_room\": \"indoor_residential\",\n",
    "    \"mansion\": \"indoor_residential\",\n",
    "    \"nursery\": \"indoor_residential\",\n",
    "    \"television_room\": \"indoor_residential\",\n",
    "    \"pantry\": \"indoor_residential\",\n",
    "    \"playroom\": \"indoor_residential\",\n",
    "    \"house\": \"indoor_residential\",\n",
    "    \"apartment_building_outdoor\": \"indoor_residential\",\n",
    "    \"balcony_interior\": \"indoor_residential\",\n",
    "    \"banquet_hall\": \"indoor_residential\",\n",
    "    \"porch\": \"indoor_residential\",\n",
    "    \"jacuzzi_indoor\": \"indoor_residential\",\n",
    "    \"shower\": \"indoor_residential\",\n",
    "    \"alcove\": \"indoor_residential\",\n",
    "    \"bedchamber\": \"indoor_residential\",\n",
    "    \"ballroom\": \"indoor_residential\",\n",
    "    \"basement\": \"indoor_residential\",\n",
    "    \"dressing_room\": \"indoor_residential\",\n",
    "    \"home_office\": \"indoor_residential\",\n",
    "    \"home_theater\": \"indoor_residential\",\n",
    "    \"manufactured_home\": \"indoor_residential\",\n",
    "    \"utility_room\": \"indoor_residential\",\n",
    "    \"wet_bar\": \"indoor_residential\",\n",
    "    \"youth_hostel\": \"indoor_residential\",\n",
    "\n",
    "    # --- Indoor commercial ---\n",
    "    \"bakery_shop\": \"indoor_commercial\",\n",
    "    \"bar\": \"indoor_commercial\",\n",
    "    \"bookstore\": \"indoor_commercial\",\n",
    "    \"butchers_shop\": \"indoor_commercial\",\n",
    "    \"candy_store\": \"indoor_commercial\",\n",
    "    \"clothing_store\": \"indoor_commercial\",\n",
    "    \"department_store\": \"indoor_commercial\",\n",
    "    \"fastfood_restaurant\": \"indoor_commercial\",\n",
    "    \"florist_shop_indoor\": \"indoor_commercial\",\n",
    "    \"gift_shop\": \"indoor_commercial\",\n",
    "    \"jewelry_shop\": \"indoor_commercial\",\n",
    "    \"market_indoor\": \"indoor_commercial\",\n",
    "    \"pet_shop\": \"indoor_commercial\",\n",
    "    \"pharmacy\": \"indoor_commercial\",\n",
    "    \"shoe_shop\": \"indoor_commercial\",\n",
    "    \"shopping_mall_indoor\": \"indoor_commercial\",\n",
    "    \"supermarket\": \"indoor_commercial\",\n",
    "    \"toyshop\": \"indoor_commercial\",\n",
    "    \"restaurant\": \"indoor_commercial\",\n",
    "    \"restaurant_kitchen\": \"indoor_commercial\",\n",
    "    \"restaurant_patio\": \"indoor_commercial\",\n",
    "    \"pizzeria\": \"indoor_commercial\",\n",
    "    \"pub_indoor\": \"indoor_commercial\",\n",
    "    \"sushi_bar\": \"indoor_commercial\",\n",
    "    \"hotel_outdoor\": \"indoor_commercial\",\n",
    "    \"amusement_arcade\": \"indoor_commercial\",\n",
    "    \"auto_showroom\": \"indoor_commercial\",\n",
    "    \"bakery_shop\": \"indoor_commercial\",\n",
    "    \"bank_vault\": \"indoor_commercial\",\n",
    "    \"bazaar_indoor\": \"indoor_commercial\",\n",
    "    \"beauty_salon\": \"indoor_commercial\",\n",
    "    \"cafeteria\": \"indoor_commercial\",\n",
    "    \"coffee_shop\": \"indoor_commercial\",\n",
    "    \"delicatessen\": \"indoor_commercial\",\n",
    "    \"discotheque\": \"indoor_commercial\",\n",
    "    \"drugstore\": \"indoor_commercial\",\n",
    "    \"fabric_store\": \"indoor_commercial\",\n",
    "    \"flea_market_indoor\": \"indoor_commercial\",\n",
    "    \"food_court\": \"indoor_commercial\",\n",
    "    \"hardware_store\": \"indoor_commercial\",\n",
    "    \"laundromat\": \"indoor_commercial\",\n",
    "    \"market_outdoor\": \"indoor_commercial\",\n",
    "    \"motel\": \"indoor_commercial\",\n",
    "    \"pub_indoor\": \"indoor_commercial\",\n",
    "    \"sauna\": \"indoor_commercial\",\n",
    "    \"server_room\": \"indoor_commercial\",\n",
    "    \"shoe_shop\": \"indoor_commercial\",\n",
    "    \"shopping_mall_indoor\": \"indoor_commercial\",\n",
    "    \"supermarket\": \"indoor_commercial\",\n",
    "    \"booth_indoor\": \"indoor_commercial\",\n",
    "    \"ice_cream_parlor\": \"indoor_commercial\",\n",
    "    \"general_store_indoor\": \"indoor_commercial\",\n",
    "    \"general_store_outdoor\": \"indoor_commercial\",\n",
    "    \"pavilion\": \"indoor_commercial\",\n",
    "    \"ticket_booth\": \"indoor_commercial\",\n",
    "\n",
    "    # --- Indoor institutional ---\n",
    "    \"art_gallery\": \"indoor_institutional\",\n",
    "    \"auditorium\": \"indoor_institutional\",\n",
    "    \"church_indoor\": \"indoor_institutional\",\n",
    "    \"classroom\": \"indoor_institutional\",\n",
    "    \"conference_room\": \"indoor_institutional\",\n",
    "    \"hospital\": \"indoor_institutional\",\n",
    "    \"hospital_room\": \"indoor_institutional\",\n",
    "    \"library_indoor\": \"indoor_institutional\",\n",
    "    \"lecture_room\": \"indoor_institutional\",\n",
    "    \"office\": \"indoor_institutional\",\n",
    "    \"office_cubicles\": \"indoor_institutional\",\n",
    "    \"schoolhouse\": \"indoor_institutional\",\n",
    "    \"science_museum\": \"indoor_institutional\",\n",
    "    \"nursing_home\": \"indoor_institutional\",\n",
    "    \"reception\": \"indoor_institutional\",\n",
    "    \"waiting_room\": \"indoor_institutional\",\n",
    "    \"museum_indoor\": \"indoor_institutional\",\n",
    "    \"biology_laboratory\": \"indoor_institutional\",\n",
    "    \"chemistry_lab\": \"indoor_institutional\",\n",
    "    \"clean_room\": \"indoor_institutional\",\n",
    "    \"conference_center\": \"indoor_institutional\",\n",
    "    \"courthouse\": \"indoor_institutional\",\n",
    "    \"embassy\": \"indoor_institutional\",\n",
    "    \"entrance_hall\": \"indoor_institutional\",\n",
    "    \"elevator_lobby\": \"indoor_institutional\",\n",
    "    \"elevator_shaft\": \"indoor_institutional\",\n",
    "    \"elevator_door\": \"indoor_institutional\",\n",
    "    \"escalator_indoor\": \"indoor_institutional\",\n",
    "    \"hospital\": \"indoor_institutional\",\n",
    "    \"kindergarden_classroom\": \"indoor_institutional\",\n",
    "    \"legislative_chamber\": \"indoor_institutional\",\n",
    "    \"library_outdoor\": \"indoor_institutional\",\n",
    "    \"lobby\": \"indoor_institutional\",\n",
    "    \"locker_room\": \"indoor_institutional\",\n",
    "    \"operating_room\": \"indoor_institutional\",\n",
    "    \"physics_laboratory\": \"indoor_institutional\",\n",
    "    \"recreation_room\": \"indoor_institutional\",\n",
    "    \"veterinarians_office\": \"indoor_institutional\",\n",
    "    \"atrium_public\": \"indoor_institutional\",\n",
    "    \"corridor\": \"indoor_institutional\",\n",
    "    \"computer_room\": \"indoor_institutional\",\n",
    "    \"mezzanine\": \"indoor_institutional\",\n",
    "    \"office_building\": \"indoor_institutional\",\n",
    "    \"jail_cell\": \"indoor_institutional\",\n",
    "    \"storage_room\": \"indoor_institutional\",\n",
    "\n",
    "    # --- Industrial facilities ---\n",
    "    \"assembly_line\": \"industrial_facility\",\n",
    "    \"auto_factory\": \"industrial_facility\",\n",
    "    \"engine_room\": \"industrial_facility\",\n",
    "    \"industrial_area\": \"industrial_facility\",\n",
    "    \"oilrig\": \"industrial_facility\",\n",
    "    \"construction_site\": \"industrial_facility\",\n",
    "    \"army_base\": \"industrial_facility\",\n",
    "    \"junkyard\": \"industrial_facility\",\n",
    "    \"repair_shop\": \"industrial_facility\",\n",
    "    \"loading_dock\": \"industrial_facility\",\n",
    "    \"fire_station\": \"industrial_facility\",\n",
    "    \"landing_deck\": \"industrial_facility\",\n",
    "    \"construction_site\": \"industrial_facility\",\n",
    "    \"gas_station\": \"industrial_facility\",\n",
    "    \"hangar_indoor\": \"industrial_facility\",\n",
    "    \"hangar_outdoor\": \"industrial_facility\",\n",
    "    \"dam\": \"industrial_facility\",\n",
    "    \"excavation\": \"industrial_facility\",\n",
    "    \"fire_escape\": \"industrial_facility\",\n",
    "    \"landfill\": \"industrial_facility\",\n",
    "    \"lock_chamber\": \"industrial_facility\",\n",
    "    \"trench\": \"industrial_facility\",\n",
    "    \"shed\": \"industrial_facility\",\n",
    "\n",
    "    # --- Urban outdoor ---\n",
    "    \"alley\": \"urban_outdoor\",\n",
    "    \"building_facade\": \"urban_outdoor\",\n",
    "    \"courtyard\": \"urban_outdoor\",\n",
    "    \"crosswalk\": \"urban_outdoor\",\n",
    "    \"downtown\": \"urban_outdoor\",\n",
    "    \"plaza\": \"urban_outdoor\",\n",
    "    \"street\": \"urban_outdoor\",\n",
    "    \"shopfront\": \"urban_outdoor\",\n",
    "    \"parking_lot\": \"urban_outdoor\",\n",
    "    \"residential_neighborhood\": \"urban_outdoor\",\n",
    "    \"skyscraper\": \"urban_outdoor\",\n",
    "    \"amusement_park\": \"urban_outdoor\",\n",
    "    \"arcade\": \"urban_outdoor\",\n",
    "    \"balcony_exterior\": \"urban_outdoor\",\n",
    "    \"bazaar_outdoor\": \"urban_outdoor\",\n",
    "    \"boardwalk\": \"urban_outdoor\",\n",
    "    \"driveway\": \"urban_outdoor\",\n",
    "    \"doorway_outdoor\": \"urban_outdoor\",\n",
    "    \"gazebo_exterior\": \"urban_outdoor\",\n",
    "    \"market_outdoor\": \"urban_outdoor\",\n",
    "    \"patio\": \"urban_outdoor\",\n",
    "    \"plaza\": \"urban_outdoor\",\n",
    "    \"promenade\": \"urban_outdoor\",\n",
    "    \"rope_bridge\": \"urban_outdoor\",\n",
    "    \"street\": \"urban_outdoor\",\n",
    "    \"tower\": \"urban_outdoor\",\n",
    "    \"slum\": \"urban_outdoor\",\n",
    "    \"sky\": \"urban_outdoor\",\n",
    "    \"roof_garden\": \"urban_outdoor\",\n",
    "    \"phone_booth\": \"urban_outdoor\",\n",
    "\n",
    "    # --- Natural landscapes ---\n",
    "    \"badlands\": \"natural_landscape\",\n",
    "    \"canyon\": \"natural_landscape\",\n",
    "    \"cliff\": \"natural_landscape\",\n",
    "    \"desert_sand\": \"natural_landscape\",\n",
    "    \"desert_vegetation\": \"natural_landscape\",\n",
    "    \"forest_broadleaf\": \"natural_landscape\",\n",
    "    \"forest_path\": \"natural_landscape\",\n",
    "    \"mountain\": \"natural_landscape\",\n",
    "    \"mountain_path\": \"natural_landscape\",\n",
    "    \"mountain_snowy\": \"natural_landscape\",\n",
    "    \"valley\": \"natural_landscape\",\n",
    "    \"volcano\": \"natural_landscape\",\n",
    "    \"rock_arch\": \"natural_landscape\",\n",
    "    \"rainforest\": \"natural_landscape\",\n",
    "    \"bamboo_forest\": \"natural_landscape\",\n",
    "    \"butte\": \"natural_landscape\",\n",
    "    \"canal_natural\": \"natural_landscape\",\n",
    "    \"cliff\": \"natural_landscape\",\n",
    "    \"coast\": \"natural_landscape\",\n",
    "    \"creek\": \"natural_landscape\",\n",
    "    \"crevasse\": \"natural_landscape\",\n",
    "    \"desert_road\": \"natural_landscape\",\n",
    "    \"field_road\": \"natural_landscape\",\n",
    "    \"forest_road\": \"natural_landscape\",\n",
    "    \"grotto\": \"natural_landscape\",\n",
    "    \"islet\": \"natural_landscape\",\n",
    "    \"lawn\": \"natural_landscape\",\n",
    "    \"marsh\": \"natural_landscape\",\n",
    "    \"swamp\": \"natural_landscape\",\n",
    "    \"vegetable_garden\": \"natural_landscape\",\n",
    "    \"wheat_field\": \"natural_landscape\",\n",
    "    \"wind_farm\": \"natural_landscape\",\n",
    "    \"windmill\": \"natural_landscape\",\n",
    "    \"zen_garden\": \"natural_landscape\",\n",
    "    \"canal_urban\": \"natural_landscape\",\n",
    "    \"fountain\": \"natural_landscape\",\n",
    "\n",
    "    # --- Cold environments ---\n",
    "    \"ice_floe\": \"cold_environment\",\n",
    "    \"ice_shelf\": \"cold_environment\",\n",
    "    \"iceberg\": \"cold_environment\",\n",
    "    \"igloo\": \"cold_environment\",\n",
    "    \"ski_resort\": \"cold_environment\",\n",
    "    \"ski_slope\": \"cold_environment\",\n",
    "    \"snowfield\": \"cold_environment\",\n",
    "    \"tundra\": \"cold_environment\",\n",
    "    \"glacier\": \"cold_environment\",\n",
    "\n",
    "    # --- Aquatic environments ---\n",
    "    \"aquarium\": \"aquatic_environment\",\n",
    "    \"beach\": \"aquatic_environment\",\n",
    "    \"boat_deck\": \"aquatic_environment\",\n",
    "    \"boathouse\": \"aquatic_environment\",\n",
    "    \"harbor\": \"aquatic_environment\",\n",
    "    \"lake_natural\": \"aquatic_environment\",\n",
    "    \"lagoon\": \"aquatic_environment\",\n",
    "    \"ocean\": \"aquatic_environment\",\n",
    "    \"pond\": \"aquatic_environment\",\n",
    "    \"river\": \"aquatic_environment\",\n",
    "    \"waterfall\": \"aquatic_environment\",\n",
    "    \"swimming_pool_indoor\": \"aquatic_environment\",\n",
    "    \"swimming_pool_outdoor\": \"aquatic_environment\",\n",
    "    \"swimming_hole\": \"aquatic_environment\",\n",
    "    \"hot_spring\": \"aquatic_environment\",\n",
    "    \"beach_house\": \"aquatic_environment\",\n",
    "    \"fishpond\": \"aquatic_environment\",\n",
    "    \"moat_water\": \"aquatic_environment\",\n",
    "    \"pier\": \"aquatic_environment\",\n",
    "    \"raft\": \"aquatic_environment\",\n",
    "    \"watering_hole\": \"aquatic_environment\",\n",
    "    \"wave\": \"aquatic_environment\",\n",
    "    \"water_tower\": \"aquatic_environment\",\n",
    "    \"underwater_ocean_deep\": \"aquatic_environment\",\n",
    "    \"lighthouse\": \"aquatic_environment\",\n",
    "\n",
    "\n",
    "    # --- Religious or historical sites ---\n",
    "    \"church_outdoor\": \"religious_or_historical_site\",\n",
    "    \"mosque_outdoor\": \"religious_or_historical_site\",\n",
    "    \"synagogue_outdoor\": \"religious_or_historical_site\",\n",
    "    \"temple_asia\": \"religious_or_historical_site\",\n",
    "    \"catacomb\": \"religious_or_historical_site\",\n",
    "    \"mausoleum\": \"religious_or_historical_site\",\n",
    "    \"palace\": \"religious_or_historical_site\",\n",
    "    \"castle\": \"religious_or_historical_site\",\n",
    "    \"ruin\": \"religious_or_historical_site\",\n",
    "    \"arch\": \"religious_or_historical_site\",\n",
    "    \"amphitheater\": \"religious_or_historical_site\",\n",
    "    \"archaelogical_excavation\": \"religious_or_historical_site\",\n",
    "    \"burial_chamber\": \"religious_or_historical_site\",\n",
    "    \"cemetery\": \"religious_or_historical_site\",\n",
    "    \"chalet\": \"religious_or_historical_site\",\n",
    "    \"kasbah\": \"religious_or_historical_site\",\n",
    "    \"medina\": \"religious_or_historical_site\",\n",
    "    \"pagoda\": \"religious_or_historical_site\",\n",
    "    \"throne_room\": \"religious_or_historical_site\",\n",
    "\n",
    "    # --- Sports and entertainment ---\n",
    "    \"arena_hockey\": \"sports_and_entertainment\",\n",
    "    \"arena_performance\": \"sports_and_entertainment\",\n",
    "    \"arena_rodeo\": \"sports_and_entertainment\",\n",
    "    \"baseball_field\": \"sports_and_entertainment\",\n",
    "    \"basketball_court_indoor\": \"sports_and_entertainment\",\n",
    "    \"boxing_ring\": \"sports_and_entertainment\",\n",
    "    \"football_field\": \"sports_and_entertainment\",\n",
    "    \"gymnasium_indoor\": \"sports_and_entertainment\",\n",
    "    \"martial_arts_gym\": \"sports_and_entertainment\",\n",
    "    \"racecourse\": \"sports_and_entertainment\",\n",
    "    \"stadium_football\": \"sports_and_entertainment\",\n",
    "    \"stadium_soccer\": \"sports_and_entertainment\",\n",
    "    \"stage_indoor\": \"sports_and_entertainment\",\n",
    "    \"stage_outdoor\": \"sports_and_entertainment\",\n",
    "    \"bowling_alley\": \"sports_and_entertainment\",\n",
    "    \"movie_theater_indoor\": \"sports_and_entertainment\",\n",
    "    \"amphitheater\": \"sports_and_entertainment\",\n",
    "    \"athletic_field_outdoor\": \"sports_and_entertainment\",\n",
    "    \"ball_pit\": \"sports_and_entertainment\",\n",
    "    \"baseball_field\": \"sports_and_entertainment\",\n",
    "    \"basketball_court_indoor\": \"sports_and_entertainment\",\n",
    "    \"bow_window_indoor\": \"sports_and_entertainment\",\n",
    "    \"boxing_ring\": \"sports_and_entertainment\",\n",
    "    \"bullring\": \"sports_and_entertainment\",\n",
    "    \"football_field\": \"sports_and_entertainment\",\n",
    "    \"raceway\": \"sports_and_entertainment\",\n",
    "    \"soccer_field\": \"sports_and_entertainment\",\n",
    "    \"stadium_baseball\": \"sports_and_entertainment\",\n",
    "    \"staircase\": \"sports_and_entertainment\",\n",
    "    \"topiary_garden\": \"sports_and_entertainment\",\n",
    "    \"water_park\": \"sports_and_entertainment\",\n",
    "    \"carrousel\": \"sports_and_entertainment\",\n",
    "    \"dining_hall\": \"sports_and_entertainment\",\n",
    "    \"diner_outdoor\": \"sports_and_entertainment\",\n",
    "    \"ice_skating_rink_indoor\": \"sports_and_entertainment\",\n",
    "    \"ice_skating_rink_outdoor\": \"sports_and_entertainment\",\n",
    "\n",
    "\n",
    "    # --- Cultural spaces ---\n",
    "    \"art_studio\": \"cultural_space\",\n",
    "    \"art_school\": \"cultural_space\",\n",
    "    \"music_studio\": \"cultural_space\",\n",
    "    \"television_studio\": \"cultural_space\",\n",
    "    \"museum_outdoor\": \"cultural_space\",\n",
    "    \"artists_loft\": \"cultural_space\",\n",
    "    \"archive\": \"cultural_space\",\n",
    "    \"galley\": \"cultural_space\",\n",
    "    \"japanese_garden\": \"cultural_space\",\n",
    "    \"natural_history_museum\": \"cultural_space\",\n",
    "    \"music_studio\": \"cultural_space\",\n",
    "    \"orchestra_pit\": \"cultural_space\",\n",
    "\n",
    "    # --- Rural or recreational area ---\n",
    "    \"aqueduct\": \"rural_or_recreational_area\",\n",
    "    \"barn\": \"rural_or_recreational_area\",\n",
    "    \"barndoor\": \"rural_or_recreational_area\",\n",
    "    \"beer_garden\": \"rural_or_recreational_area\",\n",
    "    \"beer_hall\": \"rural_or_recreational_area\",\n",
    "    \"botanical_garden\": \"rural_or_recreational_area\",\n",
    "    \"cabin_outdoor\": \"rural_or_recreational_area\",\n",
    "    \"campsite\": \"rural_or_recreational_area\",\n",
    "    \"campus\": \"rural_or_recreational_area\",\n",
    "    \"farm\": \"rural_or_recreational_area\",\n",
    "    \"field_cultivated\": \"rural_or_recreational_area\",\n",
    "    \"field_wild\": \"rural_or_recreational_area\",\n",
    "    \"golf_course\": \"rural_or_recreational_area\",\n",
    "    \"greenhouse_outdoor\": \"rural_or_recreational_area\",\n",
    "    \"hayfield\": \"rural_or_recreational_area\",\n",
    "    \"orchard\": \"rural_or_recreational_area\",\n",
    "    \"park\": \"rural_or_recreational_area\",\n",
    "    \"pasture\": \"rural_or_recreational_area\",\n",
    "    \"picnic_area\": \"rural_or_recreational_area\",\n",
    "    \"playground\": \"rural_or_recreational_area\",\n",
    "    \"rice_paddy\": \"rural_or_recreational_area\",\n",
    "    \"sandbox\": \"rural_or_recreational_area\",\n",
    "    \"stable\": \"rural_or_recreational_area\",\n",
    "    \"tree_farm\": \"rural_or_recreational_area\",\n",
    "    \"tree_house\": \"rural_or_recreational_area\",\n",
    "    \"village\": \"rural_or_recreational_area\",\n",
    "    \"vineyard\": \"rural_or_recreational_area\",\n",
    "    \"volleyball_court_outdoor\": \"rural_or_recreational_area\",\n",
    "    \"yard\": \"rural_or_recreational_area\",\n",
    "    \"corn_field\": \"rural_or_recreational_area\",\n",
    "    \"corral\": \"rural_or_recreational_area\",\n",
    "    \"cottage\": \"rural_or_recreational_area\",\n",
    "    \"formal_garden\": \"rural_or_recreational_area\",\n",
    "    \"greenhouse_indoor\": \"rural_or_recreational_area\",\n",
    "    \"hunting_lodge_outdoor\": \"rural_or_recreational_area\",\n",
    "    \"inn_outdoor\": \"rural_or_recreational_area\",\n",
    "    \"kennel_outdoor\": \"rural_or_recreational_area\",\n",
    "    \"pavilion\": \"rural_or_recreational_area\",\n",
    "    \"oast_house\": \"rural_or_recreational_area\",\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7f00d30d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'aquatic_environment',\n",
       " 'cold_environment',\n",
       " 'cultural_space',\n",
       " 'indoor_commercial',\n",
       " 'indoor_institutional',\n",
       " 'indoor_residential',\n",
       " 'industrial_facility',\n",
       " 'natural_landscape',\n",
       " 'religious_or_historical_site',\n",
       " 'rural_or_recreational_area',\n",
       " 'sports_and_entertainment',\n",
       " 'transport_infrastructure',\n",
       " 'urban_outdoor'}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "set(places365_to_13categories.values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4c5f1077",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set shape:  (13135, 10)\n",
      "Test set shape:  (3284, 10)\n"
     ]
    }
   ],
   "source": [
    "# Now create train/test splits with clean data\n",
    "from sklearn.model_selection import train_test_split\n",
    "df_train, df_test = train_test_split(df, test_size=0.2, random_state=42)\n",
    "df_train, df_test = df_train.reset_index(drop=True), df_test.reset_index(drop=True)\n",
    "print(\"Training set shape: \",df_train.shape)\n",
    "print(\"Test set shape: \",df_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ca52981c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fetching 1 files: 100%|██████████| 1/1 [00:00<00:00, 981.12it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded pretrained models\n"
     ]
    }
   ],
   "source": [
    "face_processor = AutoImageProcessor.from_pretrained(\"trpakov/vit-face-expression\", use_fast=True)\n",
    "face_model = AutoModelForImageClassification.from_pretrained(\"trpakov/vit-face-expression\")\n",
    "\n",
    "face_detector = MTCNN()\n",
    "\n",
    "scene_processor = transforms.Compose([\n",
    "    transforms.Resize((256, 256)),\n",
    "    transforms.CenterCrop(224),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                         std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "# Load pretrained Places365 ResNet18\n",
    "scene_model = models.resnet18(num_classes=365)\n",
    "weights_url = \"http://places2.csail.mit.edu/models_places365/resnet18_places365.pth.tar\"\n",
    "checkpoint = torch.hub.load_state_dict_from_url(weights_url, map_location=\"cpu\")\n",
    "state_dict = {k.replace(\"module.\", \"\"): v for k, v in checkpoint[\"state_dict\"].items()}\n",
    "scene_model.load_state_dict(state_dict)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "scene_model.to(device)\n",
    "\n",
    "face_model.to(device)\n",
    "\n",
    "print(\"Loaded pretrained models\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ab4b0913",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TruncatedFaceModel(nn.Module):\n",
    "    def __init__(self, model):\n",
    "        super().__init__()\n",
    "        self.model = model\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x will be a batch of face tensors (batch_size, 3, 224, 224)\n",
    "        device = next(self.model.parameters()).device\n",
    "        x = x.to(device)\n",
    "        \n",
    "        logits_list = []\n",
    "        for face_tensor in x:\n",
    "            # Add batch dimension for individual face\n",
    "            face_batch = face_tensor.unsqueeze(0)\n",
    "            face_output = self.model(face_batch)\n",
    "            face_logits = face_output.logits[:, :6]\n",
    "            logits_list.append(face_logits)\n",
    "\n",
    "        stacked = torch.cat(logits_list, dim=0)  # Stack along batch dimension\n",
    "        return stacked.flatten()\n",
    "    \n",
    "face_model_truncated = TruncatedFaceModel(face_model)\n",
    "face_network = Network(face_model_truncated, \"face_model\", batching = True)\n",
    "face_network.optimizer = torch.optim.Adam(face_network.parameters(), lr=lr_faces)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "4ae0c053",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SceneReductionModel(nn.Module):\n",
    "    def __init__(self, input_dim=365, hidden_dim=128, output_dim=13):\n",
    "        super().__init__()\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Linear(input_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, output_dim)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "\n",
    "scene_model_reduction = SceneReductionModel()\n",
    "scene_model_reduction.to(device)\n",
    "scene_reduction_network = Network(scene_model_reduction, \"scene_reduction\", batching = True)\n",
    "scene_reduction_network.optimizer= torch.optim.Adam(scene_reduction_network.parameters(), lr=lr_scene_reduction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "95af5ff4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TruncatedSceneModel(nn.Module):\n",
    "    def __init__(self, model, scene_reduction_model):\n",
    "        super().__init__()\n",
    "        self.model = model\n",
    "        self.scene_reduction_model = scene_reduction_model\n",
    "\n",
    "    def forward(self, x):\n",
    "        device = next(self.model.parameters()).device\n",
    "        x = x.to(device)\n",
    "        # x will be a tensor of the whole image (with the right preprocessing)\n",
    "        scene_output = self.model(x)\n",
    "        # Scene output has 365 values, we want it to be mapped to the 13 more common places\n",
    "        # That mapping happens through a neural network that will be trained during DPL training\n",
    "        scene_logits = self.scene_reduction_model(scene_output).squeeze(0)\n",
    "        return scene_logits\n",
    "    \n",
    "scene_model_truncated = TruncatedSceneModel(scene_model, scene_model_reduction)\n",
    "scene_network = Network(scene_model_truncated, \"scene_model\", batching = True)\n",
    "scene_network.optimizer = torch.optim.Adam(scene_network.parameters(), lr=lr_scenes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7a83278",
   "metadata": {},
   "outputs": [
    {
     "ename": "OSError",
     "evalue": "[Errno 22] Invalid argument: 'c:\\\\Users\\\\jarne\\\\Documents\\\\Code Masterthesis\\\\nsai_social_cognition\\\\data\\\\FindingEmo_Images\\\\Run_2/Accepting soldiers rally/1044285674_0:376:3500:2266_1000x541_80_0_0_af0a34ef90e79407bad8c469e9dd3372.jpg'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "Cell \u001b[1;32mIn [20], line 43\u001b[0m\n\u001b[0;32m     40\u001b[0m         tensor_save_path \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(save_path, \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mi\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.pt\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     41\u001b[0m         torch\u001b[38;5;241m.\u001b[39msave(stacked, tensor_save_path)\n\u001b[1;32m---> 43\u001b[0m \u001b[43mpreprocess_face_tensors\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdf_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtrain\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mface_detector\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstart_index\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m680\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     44\u001b[0m preprocess_face_tensors(df_test, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtest\u001b[39m\u001b[38;5;124m\"\u001b[39m, face_detector)\n",
      "Cell \u001b[1;32mIn [20], line 9\u001b[0m, in \u001b[0;36mpreprocess_face_tensors\u001b[1;34m(df, subset, face_detector, start_index, max_size)\u001b[0m\n\u001b[0;32m      6\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mProcessing image \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mi\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(df)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m for subset \u001b[39m\u001b[38;5;132;01m{\u001b[39;00msubset\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m      8\u001b[0m img_path \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(findingemo_dir, df\u001b[38;5;241m.\u001b[39miloc[i][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mimage_path\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[1;32m----> 9\u001b[0m img \u001b[38;5;241m=\u001b[39m \u001b[43mImage\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mopen\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimg_path\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mconvert(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRGB\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     10\u001b[0m save_path \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(findingemo_dir, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mprecomputed\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdetected_faces\u001b[39m\u001b[38;5;124m\"\u001b[39m, subset)\n\u001b[0;32m     12\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mmax\u001b[39m(img\u001b[38;5;241m.\u001b[39msize) \u001b[38;5;241m>\u001b[39m max_size:\n",
      "File \u001b[1;32mc:\\Users\\jarne\\miniconda3\\envs\\deepproblog\\lib\\site-packages\\PIL\\Image.py:3513\u001b[0m, in \u001b[0;36mopen\u001b[1;34m(fp, mode, formats)\u001b[0m\n\u001b[0;32m   3511\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_path(fp):\n\u001b[0;32m   3512\u001b[0m     filename \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mfspath(fp)\n\u001b[1;32m-> 3513\u001b[0m     fp \u001b[38;5;241m=\u001b[39m \u001b[43mbuiltins\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mopen\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilename\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mrb\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m   3514\u001b[0m     exclusive_fp \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m   3515\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[1;31mOSError\u001b[0m: [Errno 22] Invalid argument: 'c:\\\\Users\\\\jarne\\\\Documents\\\\Code Masterthesis\\\\nsai_social_cognition\\\\data\\\\FindingEmo_Images\\\\Run_2/Accepting soldiers rally/1044285674_0:376:3500:2266_1000x541_80_0_0_af0a34ef90e79407bad8c469e9dd3372.jpg'"
     ]
    }
   ],
   "source": [
    "# Presave the cropped faces as tensors already with the correct preprocessing\n",
    "# We save it as a tensor consisting of 3 tensors. Each tensor corresponding to a face\n",
    "def preprocess_face_tensors(df, subset, face_detector, start_index = 0, max_size=2048):\n",
    "    for i in range(start_index, len(df)):\n",
    "        if i % 500 == 0:\n",
    "            print(f\"Processing image {i+1}/{len(df)} for subset {subset}\")\n",
    "\n",
    "        img_path = os.path.join(findingemo_dir, df.iloc[i][\"image_path\"])\n",
    "        img = Image.open(img_path).convert(\"RGB\")\n",
    "        save_path = os.path.join(findingemo_dir, \"precomputed\", \"detected_faces\", subset)\n",
    "\n",
    "        if max(img.size) > max_size:\n",
    "            ratio = max_size / max(img.size)\n",
    "            new_size = tuple(int(dim * ratio) for dim in img.size)\n",
    "            img = img.resize(new_size, Image.Resampling.LANCZOS)\n",
    "\n",
    "        img_np = np.array(img)\n",
    "        faces = face_detector.detect_faces(img_np)\n",
    "        face_tensors_list = []\n",
    "\n",
    "        if faces:\n",
    "            faces = sorted(faces, key=lambda f: f[\"box\"][2] * f[\"box\"][3], reverse=True)[:3]\n",
    "            for fdet in faces:\n",
    "                x, y, w, h = fdet[\"box\"]\n",
    "                x, y = max(0, x), max(0, y)\n",
    "                x2, y2 = min(img_np.shape[1], x + w), min(img_np.shape[0], y + h)\n",
    "                crop = img_np[y:y2, x:x2]\n",
    "                if crop.shape[0] < 10 or crop.shape[1] < 10:\n",
    "                    continue\n",
    "                face_tensor = face_processor(images=Image.fromarray(crop), return_tensors=\"pt\")[\"pixel_values\"].squeeze(0)\n",
    "                face_tensors_list.append(face_tensor)\n",
    "\n",
    "        # Pad with zero tensors to ensure exactly 3 face tensors\n",
    "        while len(face_tensors_list) < 3:\n",
    "            face_tensors_list.append(torch.zeros(3, 224, 224))  # Same shape as processed face tensor\n",
    "\n",
    "        # Return stack of exactly 3 face tensors\n",
    "        stacked = torch.stack(face_tensors_list[:3])  # Shape: (3, 3, 224, 224)\n",
    "        # save the tensor at the save path\n",
    "        tensor_save_path = os.path.join(save_path, f\"{i}.pt\")\n",
    "        torch.save(stacked, tensor_save_path)\n",
    "    \n",
    "preprocess_face_tensors(df_train, \"train\", face_detector, start_index=680)\n",
    "preprocess_face_tensors(df_test, \"test\", face_detector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1019,
   "id": "1f1cf56d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FindingEmoFaces(object):\n",
    "    def __init__(self, df, face_detector, max_size=2048):\n",
    "        self.df = df\n",
    "        self.face_detector = face_detector\n",
    "        self.max_size = max_size\n",
    "\n",
    "    def __getitem__(self, item):\n",
    "        index = int(item[0]) if isinstance(item, (tuple, list)) else int(item)\n",
    "        img_path = os.path.join(findingemo_dir, self.df.iloc[index][\"image_path\"])\n",
    "        img = Image.open(img_path).convert(\"RGB\")\n",
    "\n",
    "        if max(img.size) > self.max_size:\n",
    "            ratio = self.max_size / max(img.size)\n",
    "            new_size = tuple(int(dim * ratio) for dim in img.size)\n",
    "            img = img.resize(new_size, Image.Resampling.LANCZOS)\n",
    "\n",
    "        img_np = np.array(img)\n",
    "        faces = self.face_detector.detect_faces(img_np)\n",
    "        face_tensors_list = []\n",
    "\n",
    "        if faces:\n",
    "            faces = sorted(faces, key=lambda f: f[\"box\"][2] * f[\"box\"][3], reverse=True)[:3]\n",
    "            for fdet in faces:\n",
    "                x, y, w, h = fdet[\"box\"]\n",
    "                x, y = max(0, x), max(0, y)\n",
    "                x2, y2 = min(img_np.shape[1], x + w), min(img_np.shape[0], y + h)\n",
    "                crop = img_np[y:y2, x:x2]\n",
    "                if crop.shape[0] < 10 or crop.shape[1] < 10:\n",
    "                    continue\n",
    "                face_tensor = face_processor(images=Image.fromarray(crop), return_tensors=\"pt\")[\"pixel_values\"].squeeze(0)\n",
    "                face_tensors_list.append(face_tensor)\n",
    "\n",
    "        # Pad with zero tensors to ensure exactly 3 face tensors\n",
    "        while len(face_tensors_list) < 3:\n",
    "            face_tensors_list.append(torch.zeros(3, 224, 224))  # Same shape as processed face tensor\n",
    "\n",
    "        # Return stack of exactly 3 face tensors\n",
    "        stacked = torch.stack(face_tensors_list[:3])  # Shape: (3, 3, 224, 224)\n",
    "        return stacked\n",
    "\n",
    "facetensors_train = FindingEmoFaces(df_train, face_detector)\n",
    "facetensors_test = FindingEmoFaces(df_test, face_detector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1020,
   "id": "1c66d43d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FindingEmoSceneLogits(object):\n",
    "    def __init__(self, df):\n",
    "        self.df = df\n",
    "\n",
    "    def __getitem__(self, item):\n",
    "        index = int(item[0]) if isinstance(item, (tuple, list)) else int(item)\n",
    "        img_path = os.path.join(findingemo_dir, self.df.iloc[index][\"image_path\"])\n",
    "        img = Image.open(img_path).convert(\"RGB\")\n",
    "\n",
    "        x = scene_processor(img).unsqueeze(0).to(device)\n",
    " \n",
    "        return x\n",
    "    \n",
    "scenetensors_train = FindingEmoSceneLogits(df_train)\n",
    "scenetensors_test = FindingEmoSceneLogits(df_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1021,
   "id": "4f11c5e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ 0.0855,  0.0221,  0.3375, -0.2059, -0.0292,  0.1417, -0.2242, -0.2522,\n",
      "        -0.1419, -0.1095, -0.4523, -0.1804,  0.0405], device='cuda:0',\n",
      "       grad_fn=<SqueezeBackward1>)\n",
      "torch.Size([13])\n"
     ]
    }
   ],
   "source": [
    "x = scenetensors_train[0]\n",
    "y = scene_model_truncated(x)\n",
    "print(y)\n",
    "print(y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1022,
   "id": "739e8ada",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([-0.2959, -2.5035,  0.5354, -0.8110,  4.0392, -1.5357, -1.0524, -2.2824,\n",
      "        -0.5259, -2.4327,  2.8070,  3.7185, -0.6956, -1.1049,  1.4585, -2.2307,\n",
      "        -0.8984,  4.0905], device='cuda:0', grad_fn=<ViewBackward0>)\n",
      "torch.Size([18])\n"
     ]
    }
   ],
   "source": [
    "x = facetensors_train[0]\n",
    "y = face_model_truncated(x)\n",
    "print(y)\n",
    "print(y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1023,
   "id": "394fada7",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Model(\n",
    "    os.path.join(prolog_dir, \"model3.pl\"),\n",
    "    [face_network, scene_reduction_network, scene_network]\n",
    ")\n",
    "model.set_engine(ExactEngine(model), cache=False)\n",
    "model.optimizer = SGD(model, param_lr=lr_model)\n",
    "\n",
    "model.add_tensor_source(\"train_face\", facetensors_train)\n",
    "model.add_tensor_source(\"train_scene\", scenetensors_train)\n",
    "model.add_tensor_source(\"test_face\", facetensors_test)\n",
    "model.add_tensor_source(\"test_scene\", scenetensors_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1024,
   "id": "1fa9bb9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FindingEmo(Dataset, TorchDataset):\n",
    "    def __init__(self, indices, df, subset_name):\n",
    "        super(FindingEmo, self).__init__()\n",
    "        self.df = df\n",
    "        self.subset_name = subset_name\n",
    "        self.data = indices\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def to_query(self, i):\n",
    "        sample_idx = self.data[i]\n",
    "        emotion_idx = self.df.iloc[sample_idx]['emotion_8_idx']\n",
    "\n",
    "        E = Var(\"E\")\n",
    "\n",
    "        goal = Term(\"final_emo\",\n",
    "                    Term(\"tensor\", Term(f\"{self.subset_name}_face\", Constant(sample_idx))),\n",
    "                    Term(\"tensor\", Term(f\"{self.subset_name}_scene\", Constant(sample_idx))),\n",
    "                    E)\n",
    "        \n",
    "        return Query(goal, substitution={E: Constant(emotion_idx)})\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1025,
   "id": "c00cb458",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_size = int(len(df_train) * training_size_perc)\n",
    "\n",
    "train_indices = list(range(0, training_size))\n",
    "test_indices = list(range(0,len(df_test)))\n",
    "\n",
    "train_set = FindingEmo(train_indices, df_train, \"train\")\n",
    "test_set = FindingEmo(test_indices, df_test, \"test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1026,
   "id": "410ad2e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "loader= DataLoader(train_set,batch_size=batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1035,
   "id": "da15a6e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Direct tensor read: torch.Size([3, 3, 224, 224])\n"
     ]
    }
   ],
   "source": [
    "# Python\n",
    "from problog.logic import Term, Constant, Var\n",
    "from deepproblog.query import Query\n",
    "\n",
    "# L = Var(\"L\")\n",
    "# goal = Term(\"face_logits\", Term(\"tensor\", Term(\"test_face\", Constant(0))), L)\n",
    "# query = Query(goal, substitution={L: L})\n",
    "# results = model.solve([query])\n",
    "# print(\"DPL result:\", results[0])\n",
    "\n",
    "# try to read tensor from solver/engine (may or may not be available depending on engine)\n",
    "t = model.get_tensor(Term(\"tensor\", Term(\"test_face\", Constant(14))))\n",
    "print(\"Direct tensor read:\", t.shape)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "deepproblog",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
