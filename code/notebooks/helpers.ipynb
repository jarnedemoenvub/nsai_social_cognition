{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e1ea0b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def save_scene_images():\n",
    "#     for idx, row in tqdm(df.iterrows(), total=len(df)):\n",
    "#         img_path = os.path.join(findingemo_dir, row['image_path'])\n",
    "#         boxes = df_boxes[df_boxes['index'] == idx][['x1', 'y1', 'x2', 'y2']].values.tolist()\n",
    "#         blurred_image = blur_faces(img_path, boxes)\n",
    "#         blurred_image = cv2.resize(blurred_image, (224, 224))\n",
    "#         save_path = os.path.join(findingemo_dir, \"scenes_5\", f\"scene_{idx}.jpg\")\n",
    "#         cv2.imwrite(save_path, blurred_image, [cv2.IMWRITE_JPEG_QUALITY, 90])\n",
    "\n",
    "# save_scene_images()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c2be6c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# faces_dir = os.path.join(findingemo_dir, \"faces\")\n",
    "# os.makedirs(faces_dir, exist_ok=True)\n",
    "\n",
    "# csv_path = os.path.join(findingemo_dir, \"face_boxes.csv\")\n",
    "\n",
    "# # Create and open CSV file for writing\n",
    "# with open(csv_path, \"w\", newline=\"\") as f:\n",
    "#     writer = csv.writer(f)\n",
    "#     # header row\n",
    "#     writer.writerow([\"index\", \"image_path\", \"face_rank\", \"x1\", \"y1\", \"x2\", \"y2\", \"score\", \"crop_path\"])\n",
    "\n",
    "#     # ========================================\n",
    "#     # Iterate over the dataframe and process each image\n",
    "#     # ========================================\n",
    "#     for idx, row in tqdm(df.iterrows(), total=len(df), desc=\"Detecting faces\"):\n",
    "#         img_path = os.path.join(findingemo_dir, row[\"image_path\"])\n",
    "\n",
    "#         try:\n",
    "#             detections = RetinaFace.detect_faces(img_path)\n",
    "#         except Exception as e:\n",
    "#             print(f\"[Warning] Could not process {img_path}: {e}\")\n",
    "#             continue\n",
    "\n",
    "#         # Skip images without faces\n",
    "#         if not detections or isinstance(detections, str):\n",
    "#             continue\n",
    "\n",
    "#         # Collect all detected faces\n",
    "#         faces = []\n",
    "#         for det in detections.values():\n",
    "#             x1, y1, x2, y2 = det[\"facial_area\"]\n",
    "#             area = (x2 - x1) * (y2 - y1)\n",
    "#             faces.append({\n",
    "#                 \"coords\": (x1, y1, x2, y2),\n",
    "#                 \"score\": det[\"score\"],\n",
    "#                 \"area\": area\n",
    "#             })\n",
    "\n",
    "#         # Sort faces by area (largest first) and keep top 3\n",
    "#         faces = sorted(faces, key=lambda x: x[\"area\"], reverse=True)[:3]\n",
    "\n",
    "#         # Crop and save top faces\n",
    "#         img = Image.open(img_path).convert(\"RGB\")\n",
    "#         for i, face in enumerate(faces):\n",
    "#             x1, y1, x2, y2 = face[\"coords\"]\n",
    "#             crop = img.crop((x1, y1, x2, y2)).resize((224, 224))\n",
    "#             crop_name = f\"img_{idx}_face_{i}.jpg\"\n",
    "#             crop_path = os.path.join(faces_dir, crop_name)\n",
    "#             crop.save(crop_path, \"JPEG\", quality=90)\n",
    "\n",
    "#             # Write bounding box info to CSV\n",
    "#             writer.writerow([\n",
    "#                 idx,                      # image index\n",
    "#                 row[\"image_path\"],        # relative image path\n",
    "#                 i,                        # face rank (0=largest)\n",
    "#                 x1, y1, x2, y2,           # bounding box coordinates\n",
    "#                 face[\"score\"],            # detection confidence\n",
    "#                 crop_path                 # saved crop file\n",
    "#             ])\n",
    "\n",
    "# print(f\"Finished! Cropped faces saved in: {faces_dir}\")\n",
    "# print(f\"Bounding box CSV saved at: {csv_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "157ce84a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def save_pretrained_logits_separately(indices, output_dir):\n",
    "#     \"\"\"\n",
    "#     Save pre-computed logits from pretrained models to separate files.\n",
    "    \n",
    "#     Args:\n",
    "#         indices: list of sample indices to process\n",
    "#         output_dir: base directory to save the logits\n",
    "#     \"\"\"\n",
    "#     # Create output directories\n",
    "#     scene_dir = os.path.join(output_dir, \"scenes\")\n",
    "#     faces_dir = os.path.join(output_dir, \"faces\")\n",
    "#     os.makedirs(scene_dir, exist_ok=True)\n",
    "#     os.makedirs(faces_dir, exist_ok=True)\n",
    "    \n",
    "#     # Set models to eval mode\n",
    "#     face_model_base.eval()\n",
    "#     scene_model_base.eval()\n",
    "    \n",
    "#     for img_idx in tqdm(indices, desc=\"Saving pretrained logits\"):\n",
    "#         # ----- PROCESS SCENE -----\n",
    "#         scene_img = scenes_dataset[(img_idx,)].to(DEVICE).unsqueeze(0)  # (1, C, H, W)\n",
    "        \n",
    "#         with torch.no_grad():\n",
    "#             scene_logits = scene_model_base(scene_img).squeeze(0)  # (365,)\n",
    "        \n",
    "#         # Save scene logits\n",
    "#         scene_file = os.path.join(scene_dir, f\"scene_{img_idx}.pt\")\n",
    "#         torch.save(scene_logits.cpu(), scene_file)\n",
    "        \n",
    "#         # ----- PROCESS FACES -----\n",
    "#         face_rows = df_boxes[df_boxes['index'] == img_idx].sort_values('face_rank')\n",
    "        \n",
    "#         for _, row in face_rows.iterrows():\n",
    "#             face_rank = int(row['face_rank'])\n",
    "#             face_tensor = faces_dataset[(img_idx, face_rank)]\n",
    "            \n",
    "#             with torch.no_grad():\n",
    "#                 face_tensor = face_tensor.to(DEVICE).unsqueeze(0)  # (1, C, H, W)\n",
    "#                 outputs = face_model_base(face_tensor)  # Returns ImageClassifierOutput\n",
    "#                 face_logits = outputs.logits.squeeze(0)  # (7,)\n",
    "            \n",
    "#             # Save face logits\n",
    "#             face_file = os.path.join(faces_dir, f\"face_{img_idx}_{face_rank}.pt\")\n",
    "#             torch.save(face_logits.cpu(), face_file)\n",
    "    \n",
    "#     print(f\"\\nLogits saved to:\")\n",
    "#     print(f\"  Scenes: {scene_dir}\")\n",
    "#     print(f\"  Faces: {faces_dir}\")\n",
    "\n",
    "# # Usage:\n",
    "# # save_pretrained_logits_separately(indices, output_dir=os.path.join(data_dir, \"pretrained_logits\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14ec1973",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def save_pretrained_logits_separately(indices, output_dir):\n",
    "#     \"\"\"\n",
    "#     Save pre-computed logits from pretrained models to separate files.\n",
    "    \n",
    "#     Args:\n",
    "#         indices: list of sample indices to process\n",
    "#         output_dir: base directory to save the logits\n",
    "#     \"\"\"\n",
    "#     # Create output directories\n",
    "#     scene_dir = os.path.join(output_dir, \"scenes\")\n",
    "#     os.makedirs(scene_dir, exist_ok=True)\n",
    "    \n",
    "#     # Set models to eval mode\n",
    "#     scene_model_base.eval()\n",
    "    \n",
    "#     for img_idx in tqdm(indices, desc=\"Saving pretrained logits\"):\n",
    "#         # ----- PROCESS SCENE -----\n",
    "#         scene_img = scenes_dataset[img_idx].to(DEVICE).unsqueeze(0)  # (1, C, H, W)\n",
    "        \n",
    "#         with torch.no_grad():\n",
    "#             scene_logits = scene_model_base(scene_img)\n",
    "#             scene_logits = scene_logits.view(scene_logits.size(0), -1)\n",
    "        \n",
    "#         # Save scene logits\n",
    "#         scene_file = os.path.join(scene_dir, f\"scene_{img_idx}.pt\")\n",
    "#         torch.save(scene_logits.cpu(), scene_file)\n",
    "        \n",
    "#     print(f\"\\nEmbeddings saved to:\")\n",
    "#     print(f\"  Scenes: {scene_dir}\")\n",
    "\n",
    "# # Usage:\n",
    "# save_pretrained_logits_separately(indices, output_dir=os.path.join(data_dir, \"pretrained_backbone\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6faea096",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Show the image but with faces blurred out\n",
    "# def blur_faces(image_path, boxes):\n",
    "#     image = cv2.imread(image_path)\n",
    "#     for box in boxes:\n",
    "#         x1, y1, x2, y2 = box\n",
    "#         face = image[y1:y2, x1:x2]\n",
    "#         blurred_face = cv2.GaussianBlur(face, (99, 99), 30)\n",
    "#         image[y1:y2, x1:x2] = blurred_face\n",
    "#     return image\n",
    "\n",
    "# img_path = os.path.join(findingemo_dir, df.iloc[9]['image_path'])\n",
    "# boxes = df_boxes[df_boxes['index'] == 9][['x1', 'y1', 'x2', 'y2']].values.tolist()\n",
    "# blurred_image = blur_faces(img_path, boxes)\n",
    "\n",
    "# plt.imshow(cv2.cvtColor(blurred_image, cv2.COLOR_BGR2RGB))\n",
    "# plt.axis('off')\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86371627",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def show_image(dataframe, index):\n",
    "#     img_path = os.path.join(findingemo_dir, dataframe.loc[index, 'image_path'])\n",
    "#     print(\"Image path:\", img_path)\n",
    "#     image = cv2.imread(img_path)\n",
    "#     image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "#     plt.imshow(image)\n",
    "#     plt.axis('off')\n",
    "#     plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89a25f1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def show_pretrained_model_predictions_on_sample(sample_idx):\n",
    "#     row = df.iloc[sample_idx]\n",
    "#     img_path = os.path.join(findingemo_dir, row['image_path'])\n",
    "#     img = Image.open(img_path).convert(\"RGB\")\n",
    "    \n",
    "#     # Convert to numpy array for drawing\n",
    "#     img_array = np.array(img)\n",
    "    \n",
    "#     # Get all face boxes for this image\n",
    "#     face_boxes = df_boxes[df_boxes['index'] == sample_idx].sort_values('face_rank')\n",
    "    \n",
    "#     # Draw bounding boxes and predictions on the image\n",
    "#     for _, box_row in face_boxes.iterrows():\n",
    "#         x1, y1, x2, y2 = int(box_row['x1']), int(box_row['y1']), int(box_row['x2']), int(box_row['y2'])\n",
    "#         face_rank = int(box_row['face_rank'])\n",
    "        \n",
    "#         model_output = torch.load(os.path.join(data_dir, \"pretrained_logits\", \"faces\", f\"face_{sample_idx}_{face_rank}.pt\")).to(DEVICE).unsqueeze(0)\n",
    "#         probs_output = torch.softmax(model_output, dim=1)\n",
    "#         probs = probs_output[0].cpu().detach().numpy()\n",
    "        \n",
    "#         # Get top prediction\n",
    "#         top_idx = np.argmax(probs)\n",
    "#         top_label = f\"{fer_classes[top_idx]}: {probs[top_idx]:.2f}\"\n",
    "        \n",
    "#         # Draw rectangle\n",
    "#         cv2.rectangle(img_array, (x1, y1), (x2, y2), (0, 255, 0), 2)\n",
    "        \n",
    "#         # Calculate text size for better positioning\n",
    "#         font = cv2.FONT_HERSHEY_SIMPLEX\n",
    "#         font_scale = 0.8\n",
    "#         thickness = 2\n",
    "#         (text_width, text_height), baseline = cv2.getTextSize(top_label, font, font_scale, thickness)\n",
    "        \n",
    "#         # Position text above the box if there's space, otherwise below\n",
    "#         if y1 - text_height - 10 > 0:\n",
    "#             text_y = y1 - 10\n",
    "#         else:\n",
    "#             text_y = y2 + text_height + 10\n",
    "        \n",
    "#         # Draw text with background for better visibility\n",
    "#         cv2.rectangle(img_array, (x1, text_y - text_height - 5), \n",
    "#                     (x1 + text_width + 5, text_y + 5), (0, 255, 0), -1)\n",
    "#         cv2.putText(img_array, top_label, (x1, text_y), \n",
    "#                     font, font_scale, (0, 0, 0), thickness)\n",
    "        \n",
    "#     # Display image with bounding boxes and predictions\n",
    "#     plt.figure(figsize=(15, 10))\n",
    "#     plt.imshow(img_array)\n",
    "#     plt.axis('off')\n",
    "#     plt.title(f\"Image Index: {sample_idx}, Valence: {row['valence']}, Arousal: {row['arousal']}, Emotion: {row['emotion']}\", fontsize=16)\n",
    "#     plt.show()\n",
    "\n",
    "#     scene_logits = torch.load(os.path.join(data_dir, \"pretrained_logits\", \"scenes\", f\"scene_{sample_idx}.pt\")).to(DEVICE)\n",
    "#     scene_probs = torch.softmax(scene_logits, dim=0)\n",
    "   \n",
    "\n",
    "#     # Show top 3 from base model (365 categories)\n",
    "#     top3_base = torch.topk(scene_probs, 3)\n",
    "#     print(\"Base model (365 categories):\")\n",
    "#     for prob, idx in zip(top3_base.values.cpu(), top3_base.indices.cpu()):\n",
    "#         print(f\"  {get_category_name(idx)}: {prob:.4f}\")\n",
    "#     print(\"\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "729d9dcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def calculate_places_priors(threshold = 0.10, indices = train_indices):\n",
    "#     scene_model_base.eval()\n",
    "#     scene_dict = {scene_name: [] for scene_name in scene_categories}\n",
    "#     for sample_idx in tqdm(indices, desc=f\"Calculating priors over {len(indices)} samples\"):\n",
    "#         scene_tensor = train_scenes_dataset[(sample_idx,)]\n",
    "#         row = df.loc[sample_idx]\n",
    "#         valence = row['valence'] + 3\n",
    "#         arousal = row['arousal']\n",
    "#         with torch.no_grad():\n",
    "#             mapped_output = scene_model_base(scene_tensor.unsqueeze(0))[0]\n",
    "#             mapped_softmax = torch.softmax(mapped_output, dim=0)\n",
    "\n",
    "#         top3_scenes = torch.topk(mapped_softmax, 3)\n",
    "        \n",
    "#         # Now extract the scene names and probabilities\n",
    "#         for prob, idx in zip(top3_scenes.values.cpu(), top3_scenes.indices.cpu()):\n",
    "#             scene_name = scene_categories[idx.item()]\n",
    "#             prob_value = prob.item()\n",
    "#             if prob_value > threshold:\n",
    "#                 scene_dict[scene_name].append((valence, arousal))\n",
    "\n",
    "#     # Take the average of valence and arousal for each scene\n",
    "#     scene_priors = {}\n",
    "#     for scene_name, va_list in scene_dict.items():\n",
    "#         if len(va_list) > 0:\n",
    "#             nr_samples = len(va_list)\n",
    "#             avg_valence = sum([va[0] for va in va_list]) / nr_samples\n",
    "#             avg_arousal = sum([va[1] for va in va_list]) / nr_samples\n",
    "#             scene_priors[scene_name] = (avg_valence, avg_arousal, nr_samples)\n",
    "#         else:\n",
    "#             scene_priors[scene_name] = (None, None, 0)\n",
    "\n",
    "#     return scene_priors\n",
    "\n",
    "# scene_priors = calculate_places_priors(threshold=0.10, indices=train_indices)\n",
    "\n",
    "# # Save scene_priors to excel\n",
    "# cluster_labels = []\n",
    "# for scene_name in scene_categories:\n",
    "#     valence, arousal, nr_samples = scene_priors[scene_name]\n",
    "#     cluster_labels.append({\n",
    "#         \"scene_category\": scene_name,\n",
    "#         \"valence\": valence,\n",
    "#         \"arousal\": arousal,\n",
    "#         \"nr_samples\": nr_samples\n",
    "#     })\n",
    "\n",
    "# df_priors = pd.DataFrame(cluster_labels)\n",
    "\n",
    "# output_path = os.path.join(data_dir, \"scene_priors.xlsx\")\n",
    "# df_priors.to_excel(output_path, index=False)\n",
    "\n",
    "# print(f\"\\nscene_priors.xlsx saved to: {output_path}\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
