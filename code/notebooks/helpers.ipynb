{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e1ea0b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def save_scene_images():\n",
    "#     for idx, row in tqdm(df.iterrows(), total=len(df)):\n",
    "#         img_path = os.path.join(findingemo_dir, row['image_path'])\n",
    "#         boxes = df_boxes[df_boxes['index'] == idx][['x1', 'y1', 'x2', 'y2']].values.tolist()\n",
    "#         blurred_image = blur_faces(img_path, boxes)\n",
    "#         blurred_image = cv2.resize(blurred_image, (224, 224))\n",
    "#         save_path = os.path.join(findingemo_dir, \"scenes_5\", f\"scene_{idx}.jpg\")\n",
    "#         cv2.imwrite(save_path, blurred_image, [cv2.IMWRITE_JPEG_QUALITY, 90])\n",
    "\n",
    "# save_scene_images()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c2be6c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# faces_dir = os.path.join(findingemo_dir, \"faces\")\n",
    "# os.makedirs(faces_dir, exist_ok=True)\n",
    "\n",
    "# csv_path = os.path.join(findingemo_dir, \"face_boxes.csv\")\n",
    "\n",
    "# # Create and open CSV file for writing\n",
    "# with open(csv_path, \"w\", newline=\"\") as f:\n",
    "#     writer = csv.writer(f)\n",
    "#     # header row\n",
    "#     writer.writerow([\"index\", \"image_path\", \"face_rank\", \"x1\", \"y1\", \"x2\", \"y2\", \"score\", \"crop_path\"])\n",
    "\n",
    "#     # ========================================\n",
    "#     # Iterate over the dataframe and process each image\n",
    "#     # ========================================\n",
    "#     for idx, row in tqdm(df.iterrows(), total=len(df), desc=\"Detecting faces\"):\n",
    "#         img_path = os.path.join(findingemo_dir, row[\"image_path\"])\n",
    "\n",
    "#         try:\n",
    "#             detections = RetinaFace.detect_faces(img_path)\n",
    "#         except Exception as e:\n",
    "#             print(f\"[Warning] Could not process {img_path}: {e}\")\n",
    "#             continue\n",
    "\n",
    "#         # Skip images without faces\n",
    "#         if not detections or isinstance(detections, str):\n",
    "#             continue\n",
    "\n",
    "#         # Collect all detected faces\n",
    "#         faces = []\n",
    "#         for det in detections.values():\n",
    "#             x1, y1, x2, y2 = det[\"facial_area\"]\n",
    "#             area = (x2 - x1) * (y2 - y1)\n",
    "#             faces.append({\n",
    "#                 \"coords\": (x1, y1, x2, y2),\n",
    "#                 \"score\": det[\"score\"],\n",
    "#                 \"area\": area\n",
    "#             })\n",
    "\n",
    "#         # Sort faces by area (largest first) and keep top 3\n",
    "#         faces = sorted(faces, key=lambda x: x[\"area\"], reverse=True)[:3]\n",
    "\n",
    "#         # Crop and save top faces\n",
    "#         img = Image.open(img_path).convert(\"RGB\")\n",
    "#         for i, face in enumerate(faces):\n",
    "#             x1, y1, x2, y2 = face[\"coords\"]\n",
    "#             crop = img.crop((x1, y1, x2, y2)).resize((224, 224))\n",
    "#             crop_name = f\"img_{idx}_face_{i}.jpg\"\n",
    "#             crop_path = os.path.join(faces_dir, crop_name)\n",
    "#             crop.save(crop_path, \"JPEG\", quality=90)\n",
    "\n",
    "#             # Write bounding box info to CSV\n",
    "#             writer.writerow([\n",
    "#                 idx,                      # image index\n",
    "#                 row[\"image_path\"],        # relative image path\n",
    "#                 i,                        # face rank (0=largest)\n",
    "#                 x1, y1, x2, y2,           # bounding box coordinates\n",
    "#                 face[\"score\"],            # detection confidence\n",
    "#                 crop_path                 # saved crop file\n",
    "#             ])\n",
    "\n",
    "# print(f\"Finished! Cropped faces saved in: {faces_dir}\")\n",
    "# print(f\"Bounding box CSV saved at: {csv_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "157ce84a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def save_pretrained_logits_separately(indices, output_dir):\n",
    "#     \"\"\"\n",
    "#     Save pre-computed logits from pretrained models to separate files.\n",
    "    \n",
    "#     Args:\n",
    "#         indices: list of sample indices to process\n",
    "#         output_dir: base directory to save the logits\n",
    "#     \"\"\"\n",
    "#     # Create output directories\n",
    "#     scene_dir = os.path.join(output_dir, \"scenes\")\n",
    "#     faces_dir = os.path.join(output_dir, \"faces\")\n",
    "#     os.makedirs(scene_dir, exist_ok=True)\n",
    "#     os.makedirs(faces_dir, exist_ok=True)\n",
    "    \n",
    "#     # Set models to eval mode\n",
    "#     face_model_base.eval()\n",
    "#     scene_model_base.eval()\n",
    "    \n",
    "#     for img_idx in tqdm(indices, desc=\"Saving pretrained logits\"):\n",
    "#         # ----- PROCESS SCENE -----\n",
    "#         scene_img = scenes_dataset[(img_idx,)].to(DEVICE).unsqueeze(0)  # (1, C, H, W)\n",
    "        \n",
    "#         with torch.no_grad():\n",
    "#             scene_logits = scene_model_base(scene_img).squeeze(0)  # (365,)\n",
    "        \n",
    "#         # Save scene logits\n",
    "#         scene_file = os.path.join(scene_dir, f\"scene_{img_idx}.pt\")\n",
    "#         torch.save(scene_logits.cpu(), scene_file)\n",
    "        \n",
    "#         # ----- PROCESS FACES -----\n",
    "#         face_rows = df_boxes[df_boxes['index'] == img_idx].sort_values('face_rank')\n",
    "        \n",
    "#         for _, row in face_rows.iterrows():\n",
    "#             face_rank = int(row['face_rank'])\n",
    "#             face_tensor = faces_dataset[(img_idx, face_rank)]\n",
    "            \n",
    "#             with torch.no_grad():\n",
    "#                 face_tensor = face_tensor.to(DEVICE).unsqueeze(0)  # (1, C, H, W)\n",
    "#                 outputs = face_model_base(face_tensor)  # Returns ImageClassifierOutput\n",
    "#                 face_logits = outputs.logits.squeeze(0)  # (7,)\n",
    "            \n",
    "#             # Save face logits\n",
    "#             face_file = os.path.join(faces_dir, f\"face_{img_idx}_{face_rank}.pt\")\n",
    "#             torch.save(face_logits.cpu(), face_file)\n",
    "    \n",
    "#     print(f\"\\nLogits saved to:\")\n",
    "#     print(f\"  Scenes: {scene_dir}\")\n",
    "#     print(f\"  Faces: {faces_dir}\")\n",
    "\n",
    "# # Usage:\n",
    "# # save_pretrained_logits_separately(indices, output_dir=os.path.join(data_dir, \"pretrained_logits\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14ec1973",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def save_pretrained_logits_separately(indices, output_dir):\n",
    "#     \"\"\"\n",
    "#     Save pre-computed logits from pretrained models to separate files.\n",
    "    \n",
    "#     Args:\n",
    "#         indices: list of sample indices to process\n",
    "#         output_dir: base directory to save the logits\n",
    "#     \"\"\"\n",
    "#     # Create output directories\n",
    "#     scene_dir = os.path.join(output_dir, \"scenes\")\n",
    "#     os.makedirs(scene_dir, exist_ok=True)\n",
    "    \n",
    "#     # Set models to eval mode\n",
    "#     scene_model_base.eval()\n",
    "    \n",
    "#     for img_idx in tqdm(indices, desc=\"Saving pretrained logits\"):\n",
    "#         # ----- PROCESS SCENE -----\n",
    "#         scene_img = scenes_dataset[img_idx].to(DEVICE).unsqueeze(0)  # (1, C, H, W)\n",
    "        \n",
    "#         with torch.no_grad():\n",
    "#             scene_logits = scene_model_base(scene_img)\n",
    "#             scene_logits = scene_logits.view(scene_logits.size(0), -1)\n",
    "        \n",
    "#         # Save scene logits\n",
    "#         scene_file = os.path.join(scene_dir, f\"scene_{img_idx}.pt\")\n",
    "#         torch.save(scene_logits.cpu(), scene_file)\n",
    "        \n",
    "#     print(f\"\\nEmbeddings saved to:\")\n",
    "#     print(f\"  Scenes: {scene_dir}\")\n",
    "\n",
    "# # Usage:\n",
    "# save_pretrained_logits_separately(indices, output_dir=os.path.join(data_dir, \"pretrained_backbone\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6faea096",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Show the image but with faces blurred out\n",
    "# def blur_faces(image_path, boxes):\n",
    "#     image = cv2.imread(image_path)\n",
    "#     for box in boxes:\n",
    "#         x1, y1, x2, y2 = box\n",
    "#         face = image[y1:y2, x1:x2]\n",
    "#         blurred_face = cv2.GaussianBlur(face, (99, 99), 30)\n",
    "#         image[y1:y2, x1:x2] = blurred_face\n",
    "#     return image\n",
    "\n",
    "# img_path = os.path.join(findingemo_dir, df.iloc[9]['image_path'])\n",
    "# boxes = df_boxes[df_boxes['index'] == 9][['x1', 'y1', 'x2', 'y2']].values.tolist()\n",
    "# blurred_image = blur_faces(img_path, boxes)\n",
    "\n",
    "# plt.imshow(cv2.cvtColor(blurred_image, cv2.COLOR_BGR2RGB))\n",
    "# plt.axis('off')\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86371627",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def show_image(dataframe, index):\n",
    "#     img_path = os.path.join(findingemo_dir, dataframe.loc[index, 'image_path'])\n",
    "#     print(\"Image path:\", img_path)\n",
    "#     image = cv2.imread(img_path)\n",
    "#     image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "#     plt.imshow(image)\n",
    "#     plt.axis('off')\n",
    "#     plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2568ae0",
   "metadata": {},
   "outputs": [],
   "source": [
    "face_val_model_dpl = CORNWrapper(face_val_model, num_classes=len(VALENCE_BINS))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd6b9576",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_va_model_predictions(model, idx, dataset):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        x, y = dataset[idx]\n",
    "        x = x.to(DEVICE).float().unsqueeze(0)  # Add batch dimension\n",
    "        y = y\n",
    "\n",
    "        probs = model(x)\n",
    "        print(probs)\n",
    "        print(\"\")\n",
    "        print(df[df.index == test_indices[idx]][['valence_bin', 'arousal_bin']])\n",
    "        show_image(df, test_indices[idx])\n",
    "\n",
    "get_va_model_predictions(face_val_model_dpl, random.randint(0, len(face_val_test_dataset)-1), face_val_test_dataset)\n",
    "# get_va_model_predictions(face_val_model_dpl, 0, face_val_test_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5e7f39a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CORNWrapper(nn.Module):\n",
    "    def __init__(self, base_model, num_classes):\n",
    "        super().__init__()\n",
    "        self.base_model = base_model\n",
    "        self.num_classes = num_classes\n",
    "\n",
    "    def forward(self, x):\n",
    "        logits = self.base_model(x)                # (B, K-1)\n",
    "        p_gt = torch.sigmoid(logits)               # threshold probs\n",
    "\n",
    "        B = logits.size(0)\n",
    "        probs = torch.zeros(B, self.num_classes, device=logits.device)\n",
    "\n",
    "        # P(y = 0)\n",
    "        probs[:, 0] = 1 - p_gt[:, 0]\n",
    "\n",
    "        # P(y = k)\n",
    "        for k in range(1, self.num_classes - 1):\n",
    "            probs[:, k] = p_gt[:, k-1] - p_gt[:, k]\n",
    "\n",
    "        # P(y = K)\n",
    "        probs[:, -1] = p_gt[:, -1]\n",
    "\n",
    "        # Fix negatives\n",
    "        probs = torch.clamp(probs, min=0)\n",
    "\n",
    "        # Normalize\n",
    "        probs = probs / probs.sum(dim=1, keepdim=True)\n",
    "\n",
    "        return probs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4b2a0a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_image(dataframe, index):\n",
    "    img_path = os.path.join(findingemo_dir, dataframe.loc[index, 'image_path'])\n",
    "    print(\"Image path:\", img_path)\n",
    "    image = cv2.imread(img_path)\n",
    "    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "    plt.imshow(image)\n",
    "    plt.axis('off')\n",
    "    plt.show()"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
