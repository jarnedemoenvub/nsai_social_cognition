{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e1ea0b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def save_scene_images():\n",
    "#     for idx, row in tqdm(df.iterrows(), total=len(df)):\n",
    "#         img_path = os.path.join(findingemo_dir, row['image_path'])\n",
    "#         boxes = df_boxes[df_boxes['index'] == idx][['x1', 'y1', 'x2', 'y2']].values.tolist()\n",
    "#         blurred_image = blur_faces(img_path, boxes)\n",
    "#         blurred_image = cv2.resize(blurred_image, (224, 224))\n",
    "#         save_path = os.path.join(findingemo_dir, \"scenes_5\", f\"scene_{idx}.jpg\")\n",
    "#         cv2.imwrite(save_path, blurred_image, [cv2.IMWRITE_JPEG_QUALITY, 90])\n",
    "\n",
    "# save_scene_images()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44133300",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def debug_query(model, pred_name, *args, substitution=None):\n",
    "#     \"\"\"Helper to test intermediate predicates\"\"\"\n",
    "#     query = Query(Term(pred_name, *args), substitution=substitution)\n",
    "#     print(f\"\\n{'='*60}\")\n",
    "#     print(f\"Testing: {pred_name}\")\n",
    "#     print(f\"{'='*60}\")\n",
    "#     model.eval()\n",
    "#     results = model.solve([query])\n",
    "#     result = results[0].result\n",
    "    \n",
    "#     # Print probabilities nicely\n",
    "#     for key, prob in sorted(result.items(), key=lambda x: -x[1]):\n",
    "#         print(f\"  {key}: {prob:.4f}\")\n",
    "    \n",
    "#     return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c2be6c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# faces_dir = os.path.join(findingemo_dir, \"faces\")\n",
    "# os.makedirs(faces_dir, exist_ok=True)\n",
    "\n",
    "# csv_path = os.path.join(findingemo_dir, \"face_boxes.csv\")\n",
    "\n",
    "# # Create and open CSV file for writing\n",
    "# with open(csv_path, \"w\", newline=\"\") as f:\n",
    "#     writer = csv.writer(f)\n",
    "#     # header row\n",
    "#     writer.writerow([\"index\", \"image_path\", \"face_rank\", \"x1\", \"y1\", \"x2\", \"y2\", \"score\", \"crop_path\"])\n",
    "\n",
    "#     # ========================================\n",
    "#     # Iterate over the dataframe and process each image\n",
    "#     # ========================================\n",
    "#     for idx, row in tqdm(df.iterrows(), total=len(df), desc=\"Detecting faces\"):\n",
    "#         img_path = os.path.join(findingemo_dir, row[\"image_path\"])\n",
    "\n",
    "#         try:\n",
    "#             detections = RetinaFace.detect_faces(img_path)\n",
    "#         except Exception as e:\n",
    "#             print(f\"[Warning] Could not process {img_path}: {e}\")\n",
    "#             continue\n",
    "\n",
    "#         # Skip images without faces\n",
    "#         if not detections or isinstance(detections, str):\n",
    "#             continue\n",
    "\n",
    "#         # Collect all detected faces\n",
    "#         faces = []\n",
    "#         for det in detections.values():\n",
    "#             x1, y1, x2, y2 = det[\"facial_area\"]\n",
    "#             area = (x2 - x1) * (y2 - y1)\n",
    "#             faces.append({\n",
    "#                 \"coords\": (x1, y1, x2, y2),\n",
    "#                 \"score\": det[\"score\"],\n",
    "#                 \"area\": area\n",
    "#             })\n",
    "\n",
    "#         # Sort faces by area (largest first) and keep top 3\n",
    "#         faces = sorted(faces, key=lambda x: x[\"area\"], reverse=True)[:3]\n",
    "\n",
    "#         # Crop and save top faces\n",
    "#         img = Image.open(img_path).convert(\"RGB\")\n",
    "#         for i, face in enumerate(faces):\n",
    "#             x1, y1, x2, y2 = face[\"coords\"]\n",
    "#             crop = img.crop((x1, y1, x2, y2)).resize((224, 224))\n",
    "#             crop_name = f\"img_{idx}_face_{i}.jpg\"\n",
    "#             crop_path = os.path.join(faces_dir, crop_name)\n",
    "#             crop.save(crop_path, \"JPEG\", quality=90)\n",
    "\n",
    "#             # Write bounding box info to CSV\n",
    "#             writer.writerow([\n",
    "#                 idx,                      # image index\n",
    "#                 row[\"image_path\"],        # relative image path\n",
    "#                 i,                        # face rank (0=largest)\n",
    "#                 x1, y1, x2, y2,           # bounding box coordinates\n",
    "#                 face[\"score\"],            # detection confidence\n",
    "#                 crop_path                 # saved crop file\n",
    "#             ])\n",
    "\n",
    "# print(f\"Finished! Cropped faces saved in: {faces_dir}\")\n",
    "# print(f\"Bounding box CSV saved at: {csv_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "157ce84a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def save_pretrained_logits_separately(indices, output_dir):\n",
    "#     \"\"\"\n",
    "#     Save pre-computed logits from pretrained models to separate files.\n",
    "    \n",
    "#     Args:\n",
    "#         indices: list of sample indices to process\n",
    "#         output_dir: base directory to save the logits\n",
    "#     \"\"\"\n",
    "#     # Create output directories\n",
    "#     scene_dir = os.path.join(output_dir, \"scenes\")\n",
    "#     faces_dir = os.path.join(output_dir, \"faces\")\n",
    "#     os.makedirs(scene_dir, exist_ok=True)\n",
    "#     os.makedirs(faces_dir, exist_ok=True)\n",
    "    \n",
    "#     # Set models to eval mode\n",
    "#     face_model_base.eval()\n",
    "#     scene_model_base.eval()\n",
    "    \n",
    "#     for img_idx in tqdm(indices, desc=\"Saving pretrained logits\"):\n",
    "#         # ----- PROCESS SCENE -----\n",
    "#         scene_img = scenes_dataset[(img_idx,)].to(DEVICE).unsqueeze(0)  # (1, C, H, W)\n",
    "        \n",
    "#         with torch.no_grad():\n",
    "#             scene_logits = scene_model_base(scene_img).squeeze(0)  # (365,)\n",
    "        \n",
    "#         # Save scene logits\n",
    "#         scene_file = os.path.join(scene_dir, f\"scene_{img_idx}.pt\")\n",
    "#         torch.save(scene_logits.cpu(), scene_file)\n",
    "        \n",
    "#         # ----- PROCESS FACES -----\n",
    "#         face_rows = df_boxes[df_boxes['index'] == img_idx].sort_values('face_rank')\n",
    "        \n",
    "#         for _, row in face_rows.iterrows():\n",
    "#             face_rank = int(row['face_rank'])\n",
    "#             face_tensor = faces_dataset[(img_idx, face_rank)]\n",
    "            \n",
    "#             with torch.no_grad():\n",
    "#                 face_tensor = face_tensor.to(DEVICE).unsqueeze(0)  # (1, C, H, W)\n",
    "#                 outputs = face_model_base(face_tensor)  # Returns ImageClassifierOutput\n",
    "#                 face_logits = outputs.logits.squeeze(0)  # (7,)\n",
    "            \n",
    "#             # Save face logits\n",
    "#             face_file = os.path.join(faces_dir, f\"face_{img_idx}_{face_rank}.pt\")\n",
    "#             torch.save(face_logits.cpu(), face_file)\n",
    "    \n",
    "#     print(f\"\\nLogits saved to:\")\n",
    "#     print(f\"  Scenes: {scene_dir}\")\n",
    "#     print(f\"  Faces: {faces_dir}\")\n",
    "\n",
    "# # Usage:\n",
    "# # save_pretrained_logits_separately(indices, output_dir=os.path.join(data_dir, \"pretrained_logits\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14ec1973",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def save_pretrained_logits_separately(indices, output_dir):\n",
    "#     \"\"\"\n",
    "#     Save pre-computed logits from pretrained models to separate files.\n",
    "    \n",
    "#     Args:\n",
    "#         indices: list of sample indices to process\n",
    "#         output_dir: base directory to save the logits\n",
    "#     \"\"\"\n",
    "#     # Create output directories\n",
    "#     scene_dir = os.path.join(output_dir, \"scenes\")\n",
    "#     os.makedirs(scene_dir, exist_ok=True)\n",
    "    \n",
    "#     # Set models to eval mode\n",
    "#     scene_model_base.eval()\n",
    "    \n",
    "#     for img_idx in tqdm(indices, desc=\"Saving pretrained logits\"):\n",
    "#         # ----- PROCESS SCENE -----\n",
    "#         scene_img = scenes_dataset[img_idx].to(DEVICE).unsqueeze(0)  # (1, C, H, W)\n",
    "        \n",
    "#         with torch.no_grad():\n",
    "#             scene_logits = scene_model_base(scene_img)\n",
    "#             scene_logits = scene_logits.view(scene_logits.size(0), -1)\n",
    "        \n",
    "#         # Save scene logits\n",
    "#         scene_file = os.path.join(scene_dir, f\"scene_{img_idx}.pt\")\n",
    "#         torch.save(scene_logits.cpu(), scene_file)\n",
    "        \n",
    "#     print(f\"\\nEmbeddings saved to:\")\n",
    "#     print(f\"  Scenes: {scene_dir}\")\n",
    "\n",
    "# # Usage:\n",
    "# save_pretrained_logits_separately(indices, output_dir=os.path.join(data_dir, \"pretrained_backbone\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6faea096",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Show the image but with faces blurred out\n",
    "# def blur_faces(image_path, boxes):\n",
    "#     image = cv2.imread(image_path)\n",
    "#     for box in boxes:\n",
    "#         x1, y1, x2, y2 = box\n",
    "#         face = image[y1:y2, x1:x2]\n",
    "#         blurred_face = cv2.GaussianBlur(face, (99, 99), 30)\n",
    "#         image[y1:y2, x1:x2] = blurred_face\n",
    "#     return image\n",
    "\n",
    "# img_path = os.path.join(findingemo_dir, df.iloc[9]['image_path'])\n",
    "# boxes = df_boxes[df_boxes['index'] == 9][['x1', 'y1', 'x2', 'y2']].values.tolist()\n",
    "# blurred_image = blur_faces(img_path, boxes)\n",
    "\n",
    "# plt.imshow(cv2.cvtColor(blurred_image, cv2.COLOR_BGR2RGB))\n",
    "# plt.axis('off')\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86371627",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def show_image(dataframe, index):\n",
    "#     img_path = os.path.join(findingemo_dir, dataframe.loc[index, 'image_path'])\n",
    "#     print(\"Image path:\", img_path)\n",
    "#     image = cv2.imread(img_path)\n",
    "#     image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "#     plt.imshow(image)\n",
    "#     plt.axis('off')\n",
    "#     plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89a25f1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def show_pretrained_model_predictions_on_sample(sample_idx):\n",
    "#     row = df.iloc[sample_idx]\n",
    "#     img_path = os.path.join(findingemo_dir, row['image_path'])\n",
    "#     img = Image.open(img_path).convert(\"RGB\")\n",
    "    \n",
    "#     # Convert to numpy array for drawing\n",
    "#     img_array = np.array(img)\n",
    "    \n",
    "#     # Get all face boxes for this image\n",
    "#     face_boxes = df_boxes[df_boxes['index'] == sample_idx].sort_values('face_rank')\n",
    "    \n",
    "#     # Draw bounding boxes and predictions on the image\n",
    "#     for _, box_row in face_boxes.iterrows():\n",
    "#         x1, y1, x2, y2 = int(box_row['x1']), int(box_row['y1']), int(box_row['x2']), int(box_row['y2'])\n",
    "#         face_rank = int(box_row['face_rank'])\n",
    "        \n",
    "#         model_output = torch.load(os.path.join(data_dir, \"pretrained_logits\", \"faces\", f\"face_{sample_idx}_{face_rank}.pt\")).to(DEVICE).unsqueeze(0)\n",
    "#         probs_output = torch.softmax(model_output, dim=1)\n",
    "#         probs = probs_output[0].cpu().detach().numpy()\n",
    "        \n",
    "#         # Get top prediction\n",
    "#         top_idx = np.argmax(probs)\n",
    "#         top_label = f\"{fer_classes[top_idx]}: {probs[top_idx]:.2f}\"\n",
    "        \n",
    "#         # Draw rectangle\n",
    "#         cv2.rectangle(img_array, (x1, y1), (x2, y2), (0, 255, 0), 2)\n",
    "        \n",
    "#         # Calculate text size for better positioning\n",
    "#         font = cv2.FONT_HERSHEY_SIMPLEX\n",
    "#         font_scale = 0.8\n",
    "#         thickness = 2\n",
    "#         (text_width, text_height), baseline = cv2.getTextSize(top_label, font, font_scale, thickness)\n",
    "        \n",
    "#         # Position text above the box if there's space, otherwise below\n",
    "#         if y1 - text_height - 10 > 0:\n",
    "#             text_y = y1 - 10\n",
    "#         else:\n",
    "#             text_y = y2 + text_height + 10\n",
    "        \n",
    "#         # Draw text with background for better visibility\n",
    "#         cv2.rectangle(img_array, (x1, text_y - text_height - 5), \n",
    "#                     (x1 + text_width + 5, text_y + 5), (0, 255, 0), -1)\n",
    "#         cv2.putText(img_array, top_label, (x1, text_y), \n",
    "#                     font, font_scale, (0, 0, 0), thickness)\n",
    "        \n",
    "#     # Display image with bounding boxes and predictions\n",
    "#     plt.figure(figsize=(15, 10))\n",
    "#     plt.imshow(img_array)\n",
    "#     plt.axis('off')\n",
    "#     plt.title(f\"Image Index: {sample_idx}, Valence: {row['valence']}, Arousal: {row['arousal']}, Emotion: {row['emotion']}\", fontsize=16)\n",
    "#     plt.show()\n",
    "\n",
    "#     scene_logits = torch.load(os.path.join(data_dir, \"pretrained_logits\", \"scenes\", f\"scene_{sample_idx}.pt\")).to(DEVICE)\n",
    "#     scene_probs = torch.softmax(scene_logits, dim=0)\n",
    "   \n",
    "\n",
    "#     # Show top 3 from base model (365 categories)\n",
    "#     top3_base = torch.topk(scene_probs, 3)\n",
    "#     print(\"Base model (365 categories):\")\n",
    "#     for prob, idx in zip(top3_base.values.cpu(), top3_base.indices.cpu()):\n",
    "#         print(f\"  {get_category_name(idx)}: {prob:.4f}\")\n",
    "#     print(\"\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "729d9dcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def calculate_places_priors(threshold = 0.10, indices = train_indices):\n",
    "#     scene_model_base.eval()\n",
    "#     scene_dict = {scene_name: [] for scene_name in scene_categories}\n",
    "#     for sample_idx in tqdm(indices, desc=f\"Calculating priors over {len(indices)} samples\"):\n",
    "#         scene_tensor = train_scenes_dataset[(sample_idx,)]\n",
    "#         row = df.loc[sample_idx]\n",
    "#         valence = row['valence'] + 3\n",
    "#         arousal = row['arousal']\n",
    "#         with torch.no_grad():\n",
    "#             mapped_output = scene_model_base(scene_tensor.unsqueeze(0))[0]\n",
    "#             mapped_softmax = torch.softmax(mapped_output, dim=0)\n",
    "\n",
    "#         top3_scenes = torch.topk(mapped_softmax, 3)\n",
    "        \n",
    "#         # Now extract the scene names and probabilities\n",
    "#         for prob, idx in zip(top3_scenes.values.cpu(), top3_scenes.indices.cpu()):\n",
    "#             scene_name = scene_categories[idx.item()]\n",
    "#             prob_value = prob.item()\n",
    "#             if prob_value > threshold:\n",
    "#                 scene_dict[scene_name].append((valence, arousal))\n",
    "\n",
    "#     # Take the average of valence and arousal for each scene\n",
    "#     scene_priors = {}\n",
    "#     for scene_name, va_list in scene_dict.items():\n",
    "#         if len(va_list) > 0:\n",
    "#             nr_samples = len(va_list)\n",
    "#             avg_valence = sum([va[0] for va in va_list]) / nr_samples\n",
    "#             avg_arousal = sum([va[1] for va in va_list]) / nr_samples\n",
    "#             scene_priors[scene_name] = (avg_valence, avg_arousal, nr_samples)\n",
    "#         else:\n",
    "#             scene_priors[scene_name] = (None, None, 0)\n",
    "\n",
    "#     return scene_priors\n",
    "\n",
    "# scene_priors = calculate_places_priors(threshold=0.10, indices=train_indices)\n",
    "\n",
    "# # Save scene_priors to excel\n",
    "# cluster_labels = []\n",
    "# for scene_name in scene_categories:\n",
    "#     valence, arousal, nr_samples = scene_priors[scene_name]\n",
    "#     cluster_labels.append({\n",
    "#         \"scene_category\": scene_name,\n",
    "#         \"valence\": valence,\n",
    "#         \"arousal\": arousal,\n",
    "#         \"nr_samples\": nr_samples\n",
    "#     })\n",
    "\n",
    "# df_priors = pd.DataFrame(cluster_labels)\n",
    "\n",
    "# output_path = os.path.join(data_dir, \"scene_priors.xlsx\")\n",
    "# df_priors.to_excel(output_path, index=False)\n",
    "\n",
    "# print(f\"\\nscene_priors.xlsx saved to: {output_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5586f229",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def get_va_distribution(model, threshold=0.01):\n",
    "#     emotion_va = {}\n",
    "#     for emotion in fe_labels:\n",
    "#         emotion_va[emotion] = {'valence': [], 'arousal': [], 'probs': []}\n",
    "\n",
    "#     for train_idx in tqdm(train_indices):\n",
    "\n",
    "#         output = debug_query(model,\n",
    "#                 \"test_combo\",\n",
    "#                 Term(\"tensor\", Term(\"face_predictions\", Constant(train_idx))),\n",
    "#                 Term(\"tensor\", Term(\"scene_predictions\", Constant(train_idx))),\n",
    "#                 Var(\"CA\"),\n",
    "#                 Var(\"CV\"))\n",
    "        \n",
    "#         emotion = df.iloc[train_idx]['emotion']\n",
    "\n",
    "#         for key, prob in output.items():\n",
    "#             cv = float(key.args[2])\n",
    "#             ca = float(key.args[3])\n",
    "#             prob_val = float(prob)\n",
    "            \n",
    "#             if prob_val >= threshold:\n",
    "#                 emotion_va[emotion]['valence'].append(cv)\n",
    "#                 emotion_va[emotion]['arousal'].append(ca)\n",
    "#                 emotion_va[emotion]['probs'].append(prob_val)  # Store probabilities\n",
    "\n",
    "#     return emotion_va\n",
    "\n",
    "\n",
    "# avg_emotion_va = {}\n",
    "# for emotion, va_values in va_distribution.items():\n",
    "#     if len(va_values['valence']) > 0:\n",
    "#         # Convert to numpy arrays\n",
    "#         valences = np.array(va_values['valence'])\n",
    "#         arousals = np.array(va_values['arousal'])\n",
    "#         probs = np.array(va_values['probs'])\n",
    "        \n",
    "#         # Normalize probabilities to sum to 1\n",
    "#         probs_normalized = probs / probs.sum()\n",
    "        \n",
    "#         # Weighted average\n",
    "#         avg_valence = np.sum(valences * probs_normalized)\n",
    "#         avg_arousal = np.sum(arousals * probs_normalized)\n",
    "#     else:\n",
    "#         avg_valence = None\n",
    "#         avg_arousal = None\n",
    "    \n",
    "#     avg_emotion_va[emotion] = (avg_valence, avg_arousal)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80be510a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def closest_emotion(CV, CA):\n",
    "#     # Extract values from Constant objects\n",
    "#     cv_val = CV.value if isinstance(CV, Constant) else CV\n",
    "#     ca_val = CA.value if isinstance(CA, Constant) else CA\n",
    "    \n",
    "#     closest_emo = None\n",
    "#     min_distance = float('inf')\n",
    "#     for emo, (EV, EA) in emotion_va_bins.items():\n",
    "#         DX = cv_val - EV\n",
    "#         DY = ca_val - EA\n",
    "#         D = (DX * DX + DY * DY) ** 0.5\n",
    "#         if D < min_distance:\n",
    "#             min_distance = D\n",
    "#             closest_emo = emo\n",
    "\n",
    "#     emo_id = LABEL_TO_ID[closest_emo]\n",
    "\n",
    "#     return Constant(emo_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82236bf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.register_foreign(closest_emotion, \"closest_emotion\", 2, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c3607f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# combined_va_centroids = {'anger': (2.4171032456116555, 3.019359600458155),\n",
    "#                         'anticipation': (2.8101052960354296, 2.90118067312196),\n",
    "#                         'disgust': (2.561769099363139, 2.781531264549843),\n",
    "#                         'fear': (2.4226634795792754, 2.7616159987215667),\n",
    "#                         'joy': (3.8179193984605413, 3.4802071510276194),\n",
    "#                         'sadness': (2.305686367051083, 2.63678745898695),\n",
    "#                         'surprise': (2.9985175076110635, 3.067898037308043),\n",
    "#                         'trust': (3.2087952323808873, 3.0928009251717663)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "290102e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# class DeviceAwareModule(nn.Module):\n",
    "#     \"\"\"Wrapper that automatically handles device placement for any module\"\"\"\n",
    "#     def __init__(self, module):\n",
    "#         super().__init__()\n",
    "#         self.module = module\n",
    "    \n",
    "#     def forward(self, *args, **kwargs):\n",
    "#         # Move all tensor inputs to model's device\n",
    "#         device = next(self.module.parameters()).device\n",
    "        \n",
    "#         def to_device(x):\n",
    "#             if isinstance(x, torch.Tensor):\n",
    "#                 return x.to(device)\n",
    "#             elif isinstance(x, (list, tuple)):\n",
    "#                 return type(x)(to_device(item) for item in x)\n",
    "#             elif isinstance(x, dict):\n",
    "#                 return {k: to_device(v) for k, v in x.items()}\n",
    "#             return x\n",
    "        \n",
    "#         args = to_device(args)\n",
    "#         kwargs = to_device(kwargs)\n",
    "#         output = self.module(*args, **kwargs)\n",
    "\n",
    "#         # Flatten output if it has extra dimensions\n",
    "#         if isinstance(output, torch.Tensor) and output.dim() == 3:\n",
    "#             output = output.squeeze(1)  # Remove the middle dimension [B, 1, C] -> [B, C]\n",
    "        \n",
    "#         return output\n",
    "    \n",
    "#     def __getattr__(self, name):\n",
    "#         try:\n",
    "#             return super().__getattr__(name)\n",
    "#         except AttributeError:\n",
    "#             return getattr(self.module, name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6c05291",
   "metadata": {},
   "outputs": [],
   "source": [
    "# class CombinationMLP(nn.Module):\n",
    "#     def __init__(self, *sizes, activation=nn.ReLU, softmax=True, batch=True):\n",
    "#         super(CombinationMLP, self).__init__()\n",
    "#         layers = []\n",
    "#         self.batch = batch\n",
    "#         for i in range(len(sizes) - 2):\n",
    "#             layers.append(nn.Linear(sizes[i], sizes[i + 1]))\n",
    "#             layers.append(activation())\n",
    "#         layers.append(nn.Linear(sizes[-2], sizes[-1]))\n",
    "#         if softmax:\n",
    "#             layers.append(nn.Softmax(-1))\n",
    "#         self.nn = nn.Sequential(*layers)\n",
    "\n",
    "#     def forward(self, face_features, scene_tensor):\n",
    "#         x = torch.cat((face_features, scene_tensor), dim=-1)\n",
    "#         if not self.batch:\n",
    "#             x = x.unsqueeze(0)\n",
    "#         x = self.nn(x)\n",
    "#         return x\n",
    "\n",
    "# positive_emotion_model_torch_raw = CombinationMLP(\n",
    "#     372,\n",
    "#     256, 128,\n",
    "#     3,\n",
    "#     activation=nn.ReLU,\n",
    "#     softmax=True,\n",
    "#     batch=True\n",
    "# )\n",
    "\n",
    "# positive_emotion_model_torch = DeviceAwareModule(positive_emotion_model_torch_raw).to(DEVICE)\n",
    "# positive_emotion_network = Network(positive_emotion_model_torch, \"positive_emotion_model\", batching=True)\n",
    "# positive_emotion_network.optimizer = optim.Adam(positive_emotion_model_torch.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef01a161",
   "metadata": {},
   "outputs": [],
   "source": [
    "# class VAtoEmotionNet(nn.Module):\n",
    "#     \"\"\"Maps combined valence-arousal to emotion\"\"\"\n",
    "#     def __init__(self, num_emotions=8):\n",
    "#         super().__init__()\n",
    "#         self.net = nn.Sequential(\n",
    "#             nn.Linear(2, 64),\n",
    "#             nn.ReLU(),\n",
    "#             nn.Linear(64, 32),\n",
    "#             nn.ReLU(),\n",
    "#             nn.Linear(32, num_emotions),\n",
    "#         )\n",
    "    \n",
    "#     def forward(self, valence, arousal):\n",
    "#         # Convert inputs to tensors if they're lists/scalars\n",
    "#         if not isinstance(valence, torch.Tensor):\n",
    "#             valence = torch.tensor(valence, dtype=torch.float32)\n",
    "#         if not isinstance(arousal, torch.Tensor):\n",
    "#             arousal = torch.tensor(arousal, dtype=torch.float32)\n",
    "\n",
    "#         # Ensure batch dimension for computation\n",
    "#         if valence.dim() == 0:\n",
    "#             valence = valence.unsqueeze(0)\n",
    "#         if arousal.dim() == 0:\n",
    "#             arousal = arousal.unsqueeze(0)\n",
    "\n",
    "#         # Stack into (batch, 2)\n",
    "#         x = torch.stack((valence, arousal), dim=-1)\n",
    "#         logits = self.net(x)  # (batch, num_emotions)\n",
    "#         probs = torch.softmax(logits, dim=-1)\n",
    "        \n",
    "#         # Squeeze batch dimension for single inputs\n",
    "#         if probs.shape[0] == 1:\n",
    "#             probs = probs.squeeze(0)  # (num_emotions,)\n",
    "        \n",
    "#         return probs\n",
    "\n",
    "# # Create the network - set batching=False\n",
    "# va_to_emotion_model = VAtoEmotionNet(num_emotions=len(fe_labels))\n",
    "# va_to_emotion_network = Network(va_to_emotion_model, \"va_to_emotion_model\", batching=False)\n",
    "# va_to_emotion_network.optimizer = optim.Adam(va_to_emotion_model.parameters(), lr=0.001)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
