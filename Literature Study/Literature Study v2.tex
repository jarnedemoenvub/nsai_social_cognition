\documentclass[12pt]{article}

\usepackage[a4paper, margin=2.5cm]{geometry}
\usepackage{graphicx}
\usepackage{amsmath, amsthm}
\usepackage{booktabs}
\usepackage{float}
\usepackage{fancyhdr}
\usepackage{titlesec}
\usepackage{caption}
\usepackage{hyperref}
\usepackage{lmodern}
\usepackage{multirow}
\usepackage{titling}
\usepackage{pgfplotstable}
\usepackage{algorithm}
\usepackage{algpseudocode}
\usepackage{xcolor}
\usepackage{tabularx}
\usepackage{amssymb}
\usepackage{enumitem}
\usepackage[most]{tcolorbox} 
\usepackage{adjustbox}
\usepackage{pdfpages}  
\usepackage[normalem]{ulem}
 \usepackage{amsmath,amssymb}
 \usepackage{tcolorbox}
 \newtcolorbox{definitionbox}[1][]{colback=gray!10!white,colframe=black,title=Definition,#1}
 \usepackage{listings}
\lstdefinelanguage{Prolog}{
  keywords={is,not,mod,div},
  morecomment=[l]\%,
  morecomment=[s]{/*}{*/},
  morestring=[b]",
}

\lstset{
  language=Prolog,
  basicstyle=\ttfamily\small,
  keywordstyle=\color{blue},
  commentstyle=\color{gray},
  stringstyle=\color{orange},
  numbers=left,
  numberstyle=\tiny\color{gray},
  stepnumber=1,
  frame=single,
  breaklines=true,
  tabsize=2,
}
\includepdfset{pagecommand=\thispagestyle{fancy}}
\tcbuselibrary{breakable} 

\pagestyle{fancy}
\fancyhf{}
\rhead{Neurosymbolic AI for Social Cognition}
\lhead{Jarne Demoen - 0616897}
\rfoot{\thepage}
\setlength{\headheight}{14.5pt}

\title{Neurosymbolic AI for Social Cognition}
\author{Jarne Demoen\\
Vrije Universiteit Brussel}
\date{4/11/2025}

\pretitle{%
  \begin{center}
  \large\textbf{Master's Thesis}\\[1ex]
}
\posttitle{\end{center}}

\begin{document}
\maketitle

\section{Introduction}
Humans effortlessly integrate facial expressions, body language, and situational cues to infer emotions and intentions. A raised eyebrow conveys doubt, while a smile can convey happiness or trust. Machines, however, have a harder time interpreting body language, context, and multi-person group structure to derive the overall meaning of a situation. Modern techniques such as machine learning and deep learning can recognize visual patterns with high accuracy, but they do so in a purely subsymbolic manner: neural networks map inputs to outputs without providing insight into how decisions are made. As a result, even if a system predicts the correct emotion of a social scene, it remains unclear why the model reached that conclusion. This lack of transparency motivates the need for learning frameworks that support both perception and reasoning.
\vspace{1em}
\newline This thesis explores how neurosymbolic AI can help machines take their next step towards genuine social cognition. Social cognition is the human ability to interpret emotions, intentions, and the subtle dynamics that shape everyday social interactions. 

Machines are increasingly expected to navigate human environments. For example, they support people at home, mediating online interactions, or working together with humans in shared physical spaces. In all of these settings, an accurate recognition of emotions is essential for safe, trustworthy, and socially aware AI systems. Yet today's emotion-recognition models remain largely pattern-driven. They could detect a smile, but they often miss whether it is a joyful smile at a wedding, a polite smile in a meeting, or a strained smile in a stressful situation. This motivates the need for approaches that integrate perceptual cues with structured, human-like knowledge. 
\vspace{1em}
\newline Neurosymbolic AI provides a promising path forward by combining two complementary paradigms:

\begin{itemize}
    \item \textbf{Symbolic AI}, which represents knowledge through a human-readable structure, such as logical rules, for example, "people are usually happy at weddings" or relationships between events.
    \item \textbf{Subsymbolic AI} (deep learning), which excels at uncovering patterns in raw data, such as recognizing facial expression in images. 
    \item \textbf{Neurosymbolic AI} which unifies both approaches by using neural networks for perception and symbolic reasoning modules to interpret structured relationships.
\end{itemize}

In this work, symbolic knowledge refers to contextual cues that describe the social setting, such as the environment type and information derived from multiple people's facial expressions. Rather than solely relying on pixel-level features, the system incorporates a structured representation of what is actually happening in the scene and who is expressing which emotion. This allows us to investigate how relationships between context and group emotional configuration may influence the final emotion prediction.
\vspace{1em}
\newline This thesis addresses the following research questions:

\begin{enumerate}
    \item Can explicit symbolic knowledge about context and group composition improve the accuracy of emotion recognition in social scenes?
    \item Can such knowledge reduce the amount of training data required compared to a purely subsymbolic baseline?
    \item Does a neurosymbolic setup make it easier to interpret and explain the modelâ€™s decisions, for example, by inspecting which rules were used?
\end{enumerate}

To study these questions, the experiments use the FindingEmo dataset, which contains naturalistic, multi-person social scenes annotated for valence, arousal, and discrete emotion categories. Its emphasis on contextual and group-based emotional understanding makes it well-suited for evaluating neurosymbolic approaches.

\section{Literature Review}
Understanding emotion in social situations requires both a description of what emotions are and an understanding of how they appear in real-world images. Research in affective science and computer vision has therefore developed several frameworks, ranging from psychological models of emotion to datasets and computational tools for learning from them.

\subsection{Psychological Models of Emotion}
One of the most important theories for representing human affect is Russell's Circumplex Model of Affect, which proposes that emotions can be mapped to a two-dimensional space defined by Valence (how positive the emotion feels) and Arousal (how intense the emotion feels). Empirical studies showed that when people judge the similarity of emotion-related terms or report their own feelings, these feelings naturally arrange themselves around a circular structure in this two-dimensional space. Emotions with similar valence and arousal values occupy nearby positions, while opposite emotions lie across the circle as shown in Figure~\ref{fig:affect8circle} . This model provides a continuous, psychologically grounded representation that is widely used in affect annotation.

\begin{figure}[H]
    \centering
    \includegraphics[width=0.6\textwidth]{Media/affect8circle.png}
    \caption{Eight affect concepts in a circular order}
    \label{fig:affect8circle}
\end{figure}

Alongside continuous representations, many emotion studies also rely on discrete taxonomies. Plutchik's Wheel of Emotions (Figure~\ref{fig:pwoe}) organizes emotions into eight basic families (such as joy, anger, fear, or trust), each within varying levels of intensity. The wheel reflects psychological opposites (such as joy and sadness) and offers a structured approach for labeling discrete emotions.

\begin{figure}[H]
    \centering
    \includegraphics[width=0.7\textwidth]{Media/plutchik.png}
    \caption{Plutchik's Wheel of Emotions (PWoE)}
    \label{fig:pwoe}
\end{figure}

\subsection{Emotion recognition in Multi-Person Social Scenes}
Traditional computer-vision benchmarks for emotion recognition have focused on isolated faces with controlled pose, lighting, and expression. However, many real-world situations involve multiple individuals, rich social contexts, and interactions. All these cues together shape the overall emotional interpretation of a scene. Recognizing emotion in such settings requires combining facial cues with contextual information about the environment.
\vspace{1em}
\newline The FindingEmo dataset was developed specifically to address this challenge. It contains images of natural social situations such as weddings, protests, sports events, family gatherings... These settings include multiple people and meaningful scenes. Each image is annotated with:

\begin{itemize}
	\item continuous Valence and Arousal values, and
	\item discrete emotion labels derived from Plutchik's taxonomy (Figure~\ref{fig:pwoe}).
\end{itemize}

This dual annotation scheme enables models to capture both broad affective tone and specific emotional categories. The dataset also highlights several challenges typical of real-world emotion understanding: class imbalance across emotion categories, greater reliability for Valence than for Arousal, and frequent confusion between semantically close emotions such as anger, fear, and disgust. Experiments with standard CNNs and vision transformers show that while facial cues and scene features both carry emotional information, simple fusion methods yield only limited improvement compared to what????. This illustrates the difficulty of reasoning across multiple cues.
\vspace{1em}
\newline This dataset, therefore, highlights the complexity of social emotion recognition: it is inherently multi-modal (faces + context), relational (multiple individuals in a single image), and approximate (human annotations include uncertainty). This motivates the interest in computational frameworks that can combine perception with structured reasoning.

\subsection{Logic-Based and Probabilistic Reasoning with Deep Learning}
One family of tools that can combine perception with structured reasoning comes from logic programming, which provides a formal way to represent knowledge and reason with it. DeepProbLog is a framework that brings this kind of reasoning together with neural networks.

\subsubsection{Prolog: Representing Knowledge Through Rules}
Prolog is a logic programming language where knowledge is expressed using facts and rules.

\begin{itemize}
	\item Facts state that something is true:
	\begin{lstlisting}
	wedding_scene(img1).
	smiling(person3).
	\end{lstlisting}

	\item Rules express implications:
	\begin{lstlisting}
	happy_event(X) :- wedding_scene(X).
	\end{lstlisting}
	Which means that X is a happy event whenever X is a wedding scene. Prolog can derive new information by chaining these rules. For example, img1 is a wedding scene, Prolog can infer that img1 is also a happy event. This symbolic 		reasoning is powerful but deterministic and does not naturally handle uncertainty or noisy predictions.
\end{itemize}

\subsection{ProbLog: Adding Probabilities to Logical Facts}
ProbLog extends Prolog by allowing facts to carry probabilities:
\begin{lstlisting}
0.8 :: smiling(person3).
0.3 :: raining(scene).
\end{lstlisting}
Which means that person3 is smiling with probability 0.8 and the scene is rainy with probability 0.3.
\vspace{1em}
\newline ProbLog also inherits Prolog's rules, so we can express:
\begin{lstlisting}
	positive_mood(X) :- smiling(X).
\end{lstlisting}


\end{document}